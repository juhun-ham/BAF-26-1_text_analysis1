{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09단원 - 자연어 처리 기초 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from nltk) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/zoohunn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/zoohunn/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 토큰화 중 생기는 선택의 순간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화1 : ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "print('단어 토큰화1 :',word_tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화2 : ['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "print('단어 토큰화2 :',WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 토큰화에서 고려해야할 사항"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트리뱅크 워드토크나이저 : ['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "text = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
    "print('트리뱅크 워드토크나이저 :',tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하이푼이 포함된 단어는 하나로 유지하고, 아포스트로피가 포함된 단어는 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 토큰화1 : ['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize # 영어 문장의 토큰화를 수행\n",
    "\n",
    "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
    "print('문장 토큰화1 :',sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 토큰화2 : ['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
     ]
    }
   ],
   "source": [
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
    "print('문장 토큰화2 :',sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kss\n",
      "  Downloading kss-6.0.6.tar.gz (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting emoji==1.2.0 (from kss)\n",
      "  Downloading emoji-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting pecab (from kss)\n",
      "  Downloading pecab-1.0.8.tar.gz (26.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: networkx in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from kss) (3.4.2)\n",
      "Collecting jamo (from kss)\n",
      "  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting hangul-jamo (from kss)\n",
      "  Downloading hangul_jamo-1.0.1-py3-none-any.whl.metadata (899 bytes)\n",
      "Collecting tossi (from kss)\n",
      "  Downloading tossi-0.3.1.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting distance (from kss)\n",
      "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyyaml>=6.0.2 (from kss)\n",
      "  Downloading pyyaml-6.0.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Collecting unidecode (from kss)\n",
      "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting cmudict (from kss)\n",
      "  Downloading cmudict-1.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting koparadigm (from kss)\n",
      "  Downloading koparadigm-0.10.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting kollocate (from kss)\n",
      "  Downloading kollocate-0.0.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting bs4 (from kss)\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from kss) (1.25.2)\n",
      "Collecting pytest (from kss)\n",
      "  Downloading pytest-9.0.2-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from kss) (1.11.4)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from bs4->kss) (4.12.3)\n",
      "Requirement already satisfied: importlib-metadata>=5 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from cmudict->kss) (8.7.0)\n",
      "Requirement already satisfied: importlib-resources>=5 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from cmudict->kss) (6.5.2)\n",
      "Collecting whoosh (from kollocate->kss)\n",
      "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting xlrd==1.2.0 (from koparadigm->kss)\n",
      "  Downloading xlrd-1.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: pyarrow in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pecab->kss) (17.0.0)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pecab->kss) (2025.9.1)\n",
      "Collecting iniconfig>=1.0.1 (from pytest->kss)\n",
      "  Downloading iniconfig-2.3.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: packaging>=22 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pytest->kss) (24.1)\n",
      "Collecting pluggy<2,>=1.5 (from pytest->kss)\n",
      "  Downloading pluggy-1.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: pygments>=2.7.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pytest->kss) (2.15.1)\n",
      "Collecting bidict (from tossi->kss)\n",
      "  Downloading bidict-0.23.1-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: six in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from tossi->kss) (1.16.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from importlib-metadata>=5->cmudict->kss) (3.23.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from beautifulsoup4->bs4->kss) (2.5)\n",
      "Downloading emoji-1.2.0-py3-none-any.whl (131 kB)\n",
      "Downloading pyyaml-6.0.3-cp311-cp311-macosx_11_0_arm64.whl (175 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading cmudict-1.1.3-py3-none-any.whl (939 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.7/939.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hangul_jamo-1.0.1-py3-none-any.whl (4.4 kB)\n",
      "Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
      "Downloading kollocate-0.0.2-py3-none-any.whl (72.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.2/72.2 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading koparadigm-0.10.0-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
      "Downloading pytest-9.0.2-py3-none-any.whl (374 kB)\n",
      "Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
      "Downloading iniconfig-2.3.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading pluggy-1.6.0-py3-none-any.whl (20 kB)\n",
      "Downloading bidict-0.23.1-py3-none-any.whl (32 kB)\n",
      "Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
      "Building wheels for collected packages: kss, distance, pecab, tossi\n",
      "  Building wheel for kss (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kss: filename=kss-6.0.6-cp311-cp311-macosx_11_0_arm64.whl size=1125676 sha256=c0257cfb67a1815635fff4b68da4b9f7593874ff350733e90c9308fd2f5469ad\n",
      "  Stored in directory: /Users/zoohunn/Library/Caches/pip/wheels/2a/ad/4b/95354758d4561a1c9386f620e17ed85f8d06c9d980ff1576b0\n",
      "  Building wheel for distance (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16256 sha256=714862c42dc4c394e34ae0760a7223d2e5556e83ea32f896691f9c03d3ea4881\n",
      "  Stored in directory: /Users/zoohunn/Library/Caches/pip/wheels/fb/cd/9c/3ab5d666e3bcacc58900b10959edd3816cc9557c7337986322\n",
      "  Building wheel for pecab (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pecab: filename=pecab-1.0.8-py3-none-any.whl size=26646664 sha256=b80d39a07a0a10c7eb2ba76cb36d31cb51ae42d3aac5f6c147789c21f6743ca5\n",
      "  Stored in directory: /Users/zoohunn/Library/Caches/pip/wheels/c9/0d/97/ca2bb361e44a80f4c63efe6f6438ff903fd1ab5640eedabc1b\n",
      "  Building wheel for tossi (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tossi: filename=tossi-0.3.1-py3-none-any.whl size=12129 sha256=3f717c8685b6babaed1a6342d513402ff98919ab9d62fc5812e0a541c913e08c\n",
      "  Stored in directory: /Users/zoohunn/Library/Caches/pip/wheels/36/1a/7e/0b78039c20678a6682f03cca4295efaa5fb55a3d10d7e9837a\n",
      "Successfully built kss distance pecab tossi\n",
      "Installing collected packages: whoosh, jamo, hangul-jamo, emoji, distance, xlrd, unidecode, pyyaml, pluggy, kollocate, iniconfig, bidict, tossi, pytest, koparadigm, cmudict, bs4, pecab, kss\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0.1\n",
      "    Uninstalling PyYAML-6.0.1:\n",
      "      Successfully uninstalled PyYAML-6.0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kubernetes 34.1.0 requires urllib3<2.4.0,>=1.24.2, but you have urllib3 2.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bidict-0.23.1 bs4-0.0.2 cmudict-1.1.3 distance-0.1.3 emoji-1.2.0 hangul-jamo-1.0.1 iniconfig-2.3.0 jamo-0.4.1 kollocate-0.0.2 koparadigm-0.10.0 kss-6.0.6 pecab-1.0.8 pluggy-1.6.0 pytest-9.0.2 pyyaml-6.0.3 tossi-0.3.1 unidecode-1.4.0 whoosh-2.7.4 xlrd-1.2.0\n"
     ]
    }
   ],
   "source": [
    "# 한국어에 대한 문장 토큰화 도구\n",
    "!pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Because there's no supported C++ morpheme analyzer, Kss will take pecab as a backend. :D\n",
      "For your information, Kss also supports mecab backend.\n",
      "We recommend you to install mecab or konlpy.tag.Mecab for faster execution of Kss.\n",
      "Please refer to following web sites for details:\n",
      "- mecab: https://github.com/hyunwoongko/python-mecab-kor\n",
      "- konlpy.tag.Mecab: https://konlpy.org/en/latest/api/konlpy.tag/#mecab-class\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 문장 토큰화 : ['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다.', '이제 해보면 알걸요?']\n"
     ]
    }
   ],
   "source": [
    "import kss\n",
    "\n",
    "text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'\n",
    "print('한국어 문장 토큰화 :',kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. NLTK와 KoNLPy를 이용한 영어, 한국어 토큰화 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화 : ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n",
      "품사 태깅 : [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
    "tokenized_sentence = word_tokenize(text)\n",
    "\n",
    "print('단어 토큰화 :',tokenized_sentence)\n",
    "print('품사 태깅 :',pos_tag(tokenized_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting JPype1>=0.7.0 (from konlpy)\n",
      "  Downloading jpype1-1.6.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (5.0 kB)\n",
      "Collecting lxml>=4.1.0 (from konlpy)\n",
      "  Downloading lxml-6.0.2-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: numpy>=1.6 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from konlpy) (1.25.2)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from JPype1>=0.7.0->konlpy) (24.1)\n",
      "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jpype1-1.6.0-cp311-cp311-macosx_10_9_universal2.whl (583 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m583.5/583.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lxml-6.0.2-cp311-cp311-macosx_10_9_universal2.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lxml, JPype1, konlpy\n",
      "Successfully installed JPype1-1.6.0 konlpy-0.6.0 lxml-6.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKT 형태소 분석 : ['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n",
      "OKT 품사 태깅 : [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n",
      "OKT 명사 추출 : ['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "okt = Okt()\n",
    "kkma = Kkma()\n",
    "\n",
    "print('OKT 형태소 분석 :',okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "print('OKT 품사 태깅 :',okt.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "print('OKT 명사 추출 :',okt.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "꼬꼬마 형태소 분석 : ['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n",
      "꼬꼬마 품사 태깅 : [('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n",
      "꼬꼬마 명사 추출 : ['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "print('꼬꼬마 형태소 분석 :',kkma.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "print('꼬꼬마 품사 태깅 :',kkma.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "print('꼬꼬마 명사 추출 :',kkma.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was wondering anyone out there could enlighten this car.\n"
     ]
    }
   ],
   "source": [
    "# 불필요한 단어 제거\n",
    "\n",
    "import re\n",
    "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
    "\n",
    "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "print(shortword.sub('', text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NLTK 불용어 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 198\n",
      "불용어 10개 출력 : ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words_list = stopwords.words('english')\n",
    "print('불용어 개수 :', len(stop_words_list))\n",
    "print('불용어 10개 출력 :',stop_words_list[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. NLTK를 통해서 불용어 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전 : ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "불용어 제거 후 : ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "불용어 제거 전 단어 개수 : 11\n",
      "불용어 제거 후 단어 개수 : 8\n"
     ]
    }
   ],
   "source": [
    "example = \"Family is not an important thing. It's everything.\"\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "word_tokens = word_tokenize(example) # 단어 토큰화\n",
    "\n",
    "result = []\n",
    "for word in word_tokens: \n",
    "    if word not in stop_words: \n",
    "        result.append(word) \n",
    "\n",
    "print('불용어 제거 전 :',word_tokens) \n",
    "print('불용어 제거 후 :',result)\n",
    "print('불용어 제거 전 단어 개수 :',len(word_tokens))\n",
    "print('불용어 제거 후 단어 개수 :',len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전 : ['Danielle', ',', 'a', 'former', 'member', 'of', 'girl', 'group', 'NewJeans', ',', 'has', 'sparked', 'speculation', 'of', 'a', 'possible', 'debut', 'in', 'China', 'after', 'opening', 'an', 'account', 'on', 'Xiaohongshu', ',', 'a', 'popular', 'Chinese', 'social', 'media', 'platform', 'similar', 'to', 'Instagram', '.']\n",
      "불용어 제거 후 : ['Danielle', ',', 'former', 'member', 'girl', 'group', 'NewJeans', ',', 'sparked', 'speculation', 'possible', 'debut', 'China', 'opening', 'account', 'Xiaohongshu', ',', 'popular', 'Chinese', 'social', 'media', 'platform', 'similar', 'Instagram', '.']\n",
      "불용어 제거 후 문장 : Danielle , former member girl group NewJeans , sparked speculation possible debut China opening account Xiaohongshu , popular Chinese social media platform similar Instagram .\n",
      "불용어 제거 전 단어 개수 : 36\n",
      "불용어 제거 후 단어 개수 : 25\n"
     ]
    }
   ],
   "source": [
    "example = \"Danielle, a former member of girl group NewJeans, has sparked speculation of a possible debut in China after opening an account on Xiaohongshu, a popular Chinese social media platform similar to Instagram.\"\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "word_tokens = word_tokenize(example) # 단어 토큰화\n",
    "\n",
    "result = []\n",
    "for word in word_tokens: \n",
    "    if word not in stop_words: \n",
    "        result.append(word) \n",
    "\n",
    "print('불용어 제거 전 :',word_tokens) \n",
    "print('불용어 제거 후 :',result)\n",
    "print('불용어 제거 후 문장 :',' '.join(result))\n",
    "print('불용어 제거 전 단어 개수 :',len(word_tokens))\n",
    "print('불용어 제거 후 단어 개수 :',len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전 : ['고기', '를', '아무렇게나', '구', '우려', '고', '하면', '안', '돼', '.', '고기', '라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살', '을', '구울', '때', '는', '중요한', '게', '있지', '.']\n",
      "불용어 제거 후 : ['고기', '하면', '.', '고기', '라고', '다', '아니거든', '.', '예컨대', '삼겹살', '을', '중요한', '있지', '.']\n"
     ]
    }
   ],
   "source": [
    "okt = Okt()\n",
    "\n",
    "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
    "stop_words = \"를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는\"\n",
    "\n",
    "stop_words = set(stop_words.split(' '))\n",
    "word_tokens = okt.morphs(example)\n",
    "\n",
    "result = [word for word in word_tokens if not word in stop_words]\n",
    "\n",
    "print('불용어 제거 전 :',word_tokens) \n",
    "print('불용어 제거 후 :',result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전 : ['BTS', '는', '한국', '에서', '출발', '한', '그룹', '이라는', '방탄소년단', '의', '정체', '성과', '마음속', '에', '크게', '자리', '잡은', '그리움', '과', '깊은', '사랑', '을', '음악', '에', '녹', '이고', '싶어', ',', '이렇게', '앨범', '제목', '을', '정', '했다고', '설명', '했습니다', '.']\n",
      "불용어 제거 후 : ['BTS', '는', '한국', '에서', '출발', '한', '방탄소년단', '의', '정체', '성과', '마음속', '에', '크게', '자리', '잡은', '그리움', '과', '깊은', '사랑', '을', '음악', '에', '녹', '이고', '싶어', ',', '앨범', '제목', '을', '정', '했다고']\n"
     ]
    }
   ],
   "source": [
    "# 예제2\n",
    "okt = Okt()\n",
    "\n",
    "example = \"BTS는 한국에서 출발한 그룹이라는 방탄소년단의 정체성과 마음속에 크게 자리 잡은 그리움과 깊은 사랑을 음악에 녹이고 싶어, 이렇게 앨범 제목을 정했다고 설명했습니다.\"\n",
    "stop_words = \"그룹 이라는 이렇게 설명 했습니다 .\"\n",
    "\n",
    "stop_words = set(stop_words.split(' '))\n",
    "word_tokens = okt.morphs(example)\n",
    "\n",
    "result = [word for word in word_tokens if not word in stop_words]\n",
    "\n",
    "print('불용어 제거 전 :',word_tokens) \n",
    "print('불용어 제거 후 :',result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어(형태소화) : {'그룹', '이렇게', '했습니다', '.', '설명', '이라는'}\n",
      "불용어 제거 전 : ['BTS', '는', '한국', '에서', '출발', '한', '그룹', '이라는', '방탄소년단', '의', '정체', '성과', '마음속', '에', '크게', '자리', '잡은', '그리움', '과', '깊은', '사랑', '을', '음악', '에', '녹', '이고', '싶어', ',', '이렇게', '앨범', '제목', '을', '정', '했다고', '설명', '했습니다', '.']\n",
      "불용어 제거 후 : ['BTS', '는', '한국', '에서', '출발', '한', '방탄소년단', '의', '정체', '성과', '마음속', '에', '크게', '자리', '잡은', '그리움', '과', '깊은', '사랑', '을', '음악', '에', '녹', '이고', '싶어', ',', '앨범', '제목', '을', '정', '했다고']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "example = \"BTS는 한국에서 출발한 그룹이라는 방탄소년단의 정체성과 마음속에 크게 자리 잡은 그리움과 깊은 사랑을 음악에 녹이고 싶어, 이렇게 앨범 제목을 정했다고 설명했습니다.\"\n",
    "stop_words_text = \"그룹이라는 이렇게 설명했습니다.\"\n",
    "\n",
    "stop_words = set(okt.morphs(stop_words_text))  # 불용어도 형태소화\n",
    "word_tokens = okt.morphs(example)\n",
    "\n",
    "result = [w for w in word_tokens if w not in stop_words]\n",
    "\n",
    "print('불용어(형태소화) :', stop_words)\n",
    "print('불용어 제거 전 :', word_tokens)\n",
    "print('불용어 제거 후 :', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정규표현식 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. . 기호 -> 임의의 한 문자와 매치\n",
    "r = re.compile(\"a.c\")\n",
    "r.search(\"kkk\") # 아무런 결과도 출력되지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.search('bdc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ? 기호 -> 0 또는 1회 등장\n",
    "r = re.compile(\"ab?c\")\n",
    "r.search(\"abbc\") # 아무런 결과도 출력되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='ac'>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"ac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. * 기호 -> 0회 이상 등장\n",
    "r = re.compile(\"ab*c\")\n",
    "r.search(\"a\") # 아무런 결과도 출력되지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='ac'>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"ac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abc\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 6), match='abbbbc'>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abbbbc\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. + 기호 -> 1회 이상 등장\n",
    "r = re.compile(\"ab+c\")\n",
    "r.search(\"ac\") # 아무런 결과도 출력되지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abc\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 6), match='abbbbc'>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abbbbc\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. ^ 기호 -> 문자열의 시작\n",
    "r = re.compile(\"^ab\")\n",
    "\n",
    "# 아무런 결과도 출력되지 않는다.\n",
    "r.search(\"bbc\")\n",
    "r.search(\"zab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='ab'>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. {숫자} 기호 -> 해당 지정한 기호에 대해 반복 횟수 지정\n",
    "r = re.compile(\"ab{2}c\")\n",
    "\n",
    "# 아무런 결과도 출력되지 않는다.\n",
    "r.search(\"ac\")\n",
    "r.search(\"abc\")\n",
    "r.search(\"abbbbbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='abbc'>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. {숫자1, 숫자2} 기호 -> 해당 지정한 기호에 대해 반복 횟수의 범위 지정\n",
    "r = re.compile(\"ab{2,8}c\")\n",
    "\n",
    "# 아무런 결과도 출력되지 않는다.\n",
    "r.search(\"ac\")\n",
    "r.search(\"abc\")\n",
    "r.search(\"abbbbbbbbbc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='abbc'>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 10), match='abbbbbbbbc'>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abbbbbbbbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. {숫자, } 기호 -> 해당 지정한 기호에 대해 최소 반복 횟수 지정\n",
    "r = re.compile(\"a{2,}bc\")\n",
    "\n",
    "# 아무런 결과도 출력되지 않는다.\n",
    "r.search(\"bc\")\n",
    "r.search(\"aa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='aabc'>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"aabc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 10), match='aaaaaaaabc'>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"aaaaaaaabc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. [] 기호 -> 그 문자들 중 하나와 매치\n",
    "r = re.compile(\"[abc]\") # [abc]는 [a-c]와 같다.\n",
    "r.search(\"zzz\") # 아무런 결과도 출력되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='a'>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='a'>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"aaaaaaa\")                                                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='b'>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"baac\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile(\"[a-z]\") # 소문자에 대한 지정\n",
    "\n",
    "# 아무런 결과도 출력되지 않는다.\n",
    "r.search(\"AAA\")\n",
    "r.search(\"111\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='a'>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"aBC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. [^문자] 기호 -> 대괄호 안의 문자들을 제외한 문자와 매치\n",
    "r = re.compile(\"[^abc]\")\n",
    "\n",
    "# 아무런 결과도 출력되지 않는다.\n",
    "r.search(\"a\")\n",
    "r.search(\"ab\") \n",
    "r.search(\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='d'>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='1'>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"1\")                                                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규 표현식 모듈 함수 예제\n",
    "# match : 문자열의 처음부터 정규식과 매치되는지 조사\n",
    "\n",
    "r = re.compile(\"ab.\")\n",
    "r.match(\"kkkabc\") # 아무런 결과도 출력되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(3, 6), match='abc'>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"kkkabc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.match(\"abckkk\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사과', '딸기', '수박', '메론', '바나나']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re.split() \n",
    "# 공백 기준 분리\n",
    "text = \"사과 딸기 수박 메론 바나나\"\n",
    "re.split(\" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사과', '딸기', '수박', '메론', '바나나']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 줄바꿈 기준 분리\n",
    "text = \"\"\"사과\n",
    "딸기\n",
    "수박\n",
    "메론\n",
    "바나나\"\"\"\n",
    "\n",
    "re.split(\"\\n\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사과', '딸기', '수박', '메론', '바나나']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# '+'를 기준으로 분리\n",
    "text = \"사과+딸기+수박+메론+바나나\"\n",
    "\n",
    "re.split(\"\\+\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['010', '1234', '1234', '30']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re.findall()\n",
    "text = \"\"\"이름 : 김철수\n",
    "전화번호 : 010 - 1234 - 1234\n",
    "나이 : 30\n",
    "성별 : 남\"\"\"\n",
    "\n",
    "re.findall(\"\\d+\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"\\d+\", \"문자열입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.sub()\n",
    "text = \"Regular expression : A regular expression, regex or regexp[1] (sometimes called a rational expression)[2][3] is, in theoretical computer science and formal language theory, a sequence of characters that define a search pattern.\"\n",
    "\n",
    "preprocessed_text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "print(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제1\n",
    "text = \"\"\"100 John    PROF\n",
    "101 James   STUD\n",
    "102 Mac   STUD\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100', 'John', 'PROF', '101', 'James', 'STUD', '102', 'Mac', 'STUD']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\s+', text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100', '101', '102']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\d+',text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J', 'P', 'R', 'O', 'F', 'J', 'S', 'T', 'U', 'D', 'M', 'S', 'T', 'U', 'D']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[A-Z]',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PROF', 'STUD', 'STUD']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[A-Z]{4}',text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John', 'James', 'Mac']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[A-Z][a-z]+',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'Mr', 'Jone', 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n",
      "[\"Don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name,', 'Mr.', \"Jone's\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "# 정규표현식 기반 토크나이저\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "text = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop\"\n",
    "\n",
    "tokenizer1 = RegexpTokenizer(\"[\\w]+\")\n",
    "tokenizer2 = RegexpTokenizer(\"\\s+\", gaps=True) # gaps=True 옵션을 주면 해당 패턴을 기준으로 토큰화\n",
    "\n",
    "print(tokenizer1.tokenize(text))\n",
    "print(tokenizer2.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09-05 딥러닝을 위한 자연어 처리 전처리 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 토큰화(Tokenization): 주어진 텍스트를 단어 또는 문자 단위로 자르는 것\n",
    "#### 영어 토큰화 도구 -> sapCy, NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_text = \"A Dog Run back corner near spare bedrooms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spaCy 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(en_text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(en_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'Dog', 'Run', 'back', 'corner', 'near', 'spare', 'bedrooms']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(en_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/zoohunn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'Dog', 'Run', 'back', 'corner', 'near', 'spare', 'bedrooms']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(en_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 띄어쓰기 토큰화 -> 영어는 띄어쓰기 단위로 토큰화 해도 단어들 간 구분이 꽤나 명확"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'Dog', 'Run', 'back', 'corner', 'near', 'spare', 'bedrooms']\n"
     ]
    }
   ],
   "source": [
    "print(en_text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한국어 띄어쓰기 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_text = \"사과의 놀라운 효능이라는 글을 봤어. 그래서 오늘 사과를 먹으려고 했는데 사과가 썩어서 슈퍼에 가서 사과랑 오렌지 사왔어\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['사과의', '놀라운', '효능이라는', '글을', '봤어.', '그래서', '오늘', '사과를', '먹으려고', '했는데', '사과가', '썩어서', '슈퍼에', '가서', '사과랑', '오렌지', '사왔어']\n"
     ]
    }
   ],
   "source": [
    "print(kor_text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "조사가 모두 명사와 붙어 있음 -> 사과의, 사과를, 사과가, 사과랑 -> 이 조사들을 제거하지 않으면 다른 단어로 인식하게 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 형태소 토큰화\n",
    "#### -> 앞선 상황처럼 띄어쓰기가 잘 되어 있지 않은 한국어 문장에 대해서는 형태소 단위로 토큰화를 수행해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from konlpy) (1.6.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from konlpy) (6.0.2)\n",
      "Requirement already satisfied: numpy>=1.6 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from konlpy) (2.4.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from JPype1>=0.7.0->konlpy) (24.1)\n",
      "Requirement already satisfied: mecab-python in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (1.0.0)\n",
      "Requirement already satisfied: mecab-python3 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from mecab-python) (1.0.12)\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy\n",
    "!pip install mecab-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', ' ', 'D', 'o', 'g', ' ', 'R', 'u', 'n', ' ', 'b', 'a', 'c', 'k', ' ', 'c', 'o', 'r', 'n', 'e', 'r', ' ', 'n', 'e', 'a', 'r', ' ', 's', 'p', 'a', 'r', 'e', ' ', 'b', 'e', 'd', 'r', 'o', 'o', 'm', 's']\n"
     ]
    }
   ],
   "source": [
    "# 문자 토큰화\n",
    "print(list(en_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 단어 집합(Vocabulary) 생성\n",
    "#### 단어집합: 중복을 제거한 텍스트의 총 단어 집합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습: 네이버 영화 리뷰 데이터를 통해 리뷰 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request           # 웹에서 파일을 다운로드하기 위한 라이브러리\n",
    "import pandas as pd\n",
    "from konlpy.tag import Mecab\n",
    "from nltk import FreqDist       # 단어의 빈도수를 계산하기 위한 라이브러리\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2190435</td>\n",
       "      <td>사랑을 해본사람이라면 처음부터 끝까지 웃을수 있는영화</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9279041</td>\n",
       "      <td>완전 감동입니다 다시봐도 감동</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7865729</td>\n",
       "      <td>개들의 전쟁2 나오나요? 나오면 1빠로 보고 싶음</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7477618</td>\n",
       "      <td>굿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9250537</td>\n",
       "      <td>바보가 아니라 병 쉰 인듯</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
       "2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
       "3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
       "4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1\n",
       "5   2190435                      사랑을 해본사람이라면 처음부터 끝까지 웃을수 있는영화      1\n",
       "6   9279041                                   완전 감동입니다 다시봐도 감동      1\n",
       "7   7865729                        개들의 전쟁2 나오나요? 나오면 1빠로 보고 싶음      1\n",
       "8   7477618                                                  굿      1\n",
       "9   9250537                                     바보가 아니라 병 쉰 인듯      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")\n",
    "data = pd.read_table('ratings.txt') # 데이터프레임에 저장\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수: 200000\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 수: {}'.format(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = data[:100]  # 샘플로 100개만 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정규 표현식을 통한 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ft/qy0hfkvd0rx250sfnxdbjr7r0000gn/T/ipykernel_12167/701937034.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample_data['document'] = sample_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", regex=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2190435</td>\n",
       "      <td>사랑을 해본사람이라면 처음부터 끝까지 웃을수 있는영화</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9279041</td>\n",
       "      <td>완전 감동입니다 다시봐도 감동</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7865729</td>\n",
       "      <td>개들의 전쟁 나오나요 나오면 빠로 보고 싶음</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7477618</td>\n",
       "      <td>굿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9250537</td>\n",
       "      <td>바보가 아니라 병 쉰 인듯</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업...      1\n",
       "2   4655635                   폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고      1\n",
       "3   9251303   와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지      1\n",
       "4  10067386                         안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화      1\n",
       "5   2190435                      사랑을 해본사람이라면 처음부터 끝까지 웃을수 있는영화      1\n",
       "6   9279041                                   완전 감동입니다 다시봐도 감동      1\n",
       "7   7865729                           개들의 전쟁 나오나요 나오면 빠로 보고 싶음      1\n",
       "8   7477618                                                  굿      1\n",
       "9   9250537                                     바보가 아니라 병 쉰 인듯      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data['document'] = sample_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", regex=True) \n",
    "# [^ ]는 괄호 안의 문자들을 제외한 문자를 의미, 즉 한글과 공백이 아닌 모든 문자를 제거\n",
    "# regex=True 를 통해 앞의 패턴을 정규 표현식으로 사용\n",
    "sample_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 토큰화 수행 -> 불용어 제거 위한 불용어 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 정의\n",
    "stopwords=['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기 -> mecab 사용\n",
    "tokenizer = Mecab(dicpath=\"/opt/homebrew/lib/mecab/dic/mecab-ko-dic\")\n",
    "\n",
    "tokenized=[]\n",
    "for sentence in sample_data['document']:\n",
    "    temp = tokenizer.morphs(sentence) # 토큰화\n",
    "    temp = [word for word in temp if not word in stopwords] # 불용어 제거\n",
    "    tokenized.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['어릴', '때', '보', '고', '지금', '다시', '봐도', '재밌', '어요', 'ㅋㅋ'], ['디자인', '을', '배우', '학생', '외국', '디자이너', '그', '일군', '전통', '을', '통해', '발전', '해', '문화', '산업', '부러웠', '는데', '사실', '우리', '나라', '에서', '그', '어려운', '시절', '끝', '까지', '열정', '을', '지킨', '노라노', '같', '전통', '있', '어', '저', '같', '사람', '꿈', '을', '꾸', '고', '이뤄나갈', '수', '있', '다는', '것', '감사', '합니다'], ['폴리스', '스토리', '시리즈', '부터', '뉴', '까지', '버릴', '께', '하나', '없', '음', '최고'], ['연기', '진짜', '개', '쩔', '구나', '지루', '할거', '라고', '생각', '했', '는데', '몰입', '해서', '봤', '다', '그래', '이런', '게', '진짜', '영화', '지'], ['안개', '자욱', '밤하늘', '떠', '있', '초승달', '같', '영화'], ['사랑', '을', '해', '본', '사람', '라면', '처음', '부터', '끝', '까지', '웃', '을', '수', '있', '영화'], ['완전', '감동', '입니다', '다시', '봐도', '감동'], ['개', '전쟁', '나오', '나요', '나오', '면', '빠', '로', '보', '고', '싶', '음'], ['굿'], ['바보', '아니', '라', '병', '쉰', '인', '듯']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 664\n"
     ]
    }
   ],
   "source": [
    "# 단어 집합 생성\n",
    "vocab = FreqDist(np.hstack(tokenized)) # np.hstack()을 통해 1차원 배열로 변환, FreqDist()로 단어의 빈도수 계산\n",
    "print('단어 집합의 크기 : {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({np.str_('다'): 46, np.str_('영화'): 31, np.str_('고'): 27, np.str_('을'): 25, np.str_('하'): 21, np.str_('있'): 17, np.str_('게'): 17, np.str_('보'): 15, np.str_('없'): 13, np.str_('최고'): 13, ...})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab # 단어를 키로, 빈도수가 값으로 저장되어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['재밌']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기: 500\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 500\n",
    "# 상위 vocab_size개의 단어만 보존\n",
    "vocab = vocab.most_common(vocab_size) # most_common() 메서드를 통해 상위 빈도수 단어 추출\n",
    "print('단어 집합의 크기: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 각 단어에 고유한 정수 부여 -> 2~501까지 각 단어에 고유한 정수 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {word[0]: index+2 for index, word in enumerate(vocab)}\n",
    "word_to_index['pad'] = 1 \n",
    "word_to_index['unk'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = []\n",
    "for line in tokenized: #입력 데이터에서 1줄씩 문장을 읽음\n",
    "    temp = []\n",
    "    for w in line: #각 줄에서 1개씩 글자를 읽음\n",
    "      try:\n",
    "        temp.append(word_to_index[w]) # 글자를 해당되는 정수로 변환\n",
    "      except KeyError: # 단어 집합에 없는 단어일 경우 unk로 대체된다.\n",
    "        temp.append(word_to_index['unk']) # unk의 인덱스로 변환\n",
    "\n",
    "    encoded.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[79, 27, 9, 4, 50, 42, 80, 16, 28, 29], [188, 5, 81, 189, 190, 191, 43, 192, 113, 5, 193, 194, 24, 114, 195, 196, 13, 51, 82, 115, 30, 43, 197, 116, 117, 31, 198, 5, 199, 200, 17, 113, 7, 68, 52, 17, 44, 201, 5, 202, 4, 203, 14, 7, 83, 32, 204, 84], [205, 118, 206, 53, 207, 31, 208, 209, 54, 10, 25, 11], [45, 33, 119, 210, 211, 212, 213, 69, 46, 34, 13, 214, 120, 15, 2, 215, 70, 8, 33, 3, 35], [216, 217, 218, 219, 7, 220, 17, 3], [121, 5, 24, 36, 44, 122, 123, 53, 117, 31, 85, 5, 14, 7, 3], [124, 37, 221, 42, 80, 37], [119, 222, 55, 223, 55, 86, 224, 38, 9, 4, 47, 25], [56], [225, 87, 88, 226, 227, 57, 89], [21, 228, 17, 3, 50, 36, 48, 37, 18, 2, 229, 230, 42, 231, 232, 54, 54, 43, 125, 5, 233, 6, 8, 234, 19, 32, 22, 17, 2], [16, 2], [235, 236, 237, 2, 238, 29], [3, 239, 126, 69, 127, 19, 128, 240, 18, 241, 242, 243, 14, 10, 89], [244, 12, 71, 245], [72, 58, 90, 39, 129, 246, 33, 11, 91, 33, 247, 30, 248, 249, 250, 6, 8, 251], [59, 252, 253, 130, 254, 22, 17, 5, 255, 256, 257, 10, 25], [258, 11], [86, 131, 132, 259, 40, 260, 261], [68, 262, 15, 13, 263, 264, 265, 3, 60, 21, 11, 38, 61, 20, 2, 266, 267, 268], [269, 36, 270, 73, 271, 272], [273, 27, 7, 20, 2, 121, 274, 87, 4, 275, 12, 39, 276, 277, 62, 5, 278, 74, 279, 92, 280, 133, 281, 93, 27, 282, 283, 284, 285, 134, 286, 287, 288, 289, 135, 290, 2, 291, 35, 292, 61, 293, 57, 63, 2], [94, 136, 5, 137, 5, 14, 10, 2], [294, 18, 138, 295, 18, 138, 95, 139], [296, 140, 96, 56, 72, 58, 297], [141, 298, 299, 300, 301, 142, 74, 143, 302, 303, 9, 4, 47, 2, 144, 4, 304, 305, 306, 307, 116, 308, 94, 85, 309, 31, 23, 310, 75, 311, 145, 97, 11, 96, 2], [44, 312, 8, 313, 314, 5, 19, 14, 7, 98, 315, 9, 21, 2, 316, 317], [318, 319, 73, 320, 321, 146, 141, 322, 75, 30, 323, 24, 147, 2, 324, 325, 5, 326, 6, 76, 327, 328, 6, 26, 2], [11, 2, 329, 148, 330, 331, 332, 133, 333, 76, 15, 2, 334, 134, 335], [336, 337, 132, 338, 42, 99, 30, 339, 92, 100, 97, 340, 5, 341, 77, 3], [342, 343, 344, 128, 122, 345, 346, 101, 347, 7, 35], [348, 57, 114, 95, 149, 349, 84], [102, 350, 5, 27, 351, 16, 8, 150, 29, 151, 352, 29, 29, 45, 353], [103, 152, 354, 37, 18, 20, 2], [56, 56, 56, 104, 153, 104, 153, 355, 356], [39, 64, 2, 62, 41, 357], [358, 359, 360, 361, 154, 6, 45, 65, 362], [9, 155, 363, 364, 156, 105, 156, 105, 34, 93, 3], [365, 58, 366, 367, 368, 157, 55, 369, 29, 106, 7, 25], [370, 371, 75, 372, 75, 373, 89, 107, 3, 374, 375, 376, 377], [378, 379, 380, 381, 382, 383, 384], [3, 9, 86, 385, 4, 47, 61, 386, 32, 2], [387, 108, 66, 3, 155, 61, 388, 2, 389, 21, 12, 39, 6, 390, 391], [392, 393, 394, 395, 396], [23, 64, 8, 36, 3, 26, 2, 397, 11], [21, 108, 88, 158, 102, 398, 129, 7, 4, 159, 6, 399, 160, 160, 48, 22, 158, 400, 401, 136, 5, 402, 161, 403, 404, 46, 405, 406, 30, 407, 162, 38, 408, 16, 20, 48, 150, 102, 409, 3, 410, 163, 4, 411, 412, 151, 7, 413, 414, 29, 415, 41, 16, 8, 15, 25], [164, 19, 78, 164, 6, 416, 60, 39, 417, 418, 6, 4, 12, 49, 58, 419, 4, 420, 421, 422, 68, 423, 53, 424, 425, 4, 426, 88, 427, 428, 17, 12, 429, 60, 39], [137, 5, 14, 10, 43, 430, 431, 109, 4, 432, 433, 434, 435, 74, 436, 84], [437, 438, 161, 439, 165, 110, 18, 440, 441, 3, 26, 28], [166, 41, 12, 167, 111], [23, 442, 443, 444, 2, 81, 24, 445, 59, 19, 446, 38, 45, 168], [23, 169, 170, 8, 447, 99, 92, 448, 5, 14, 449, 450], [451, 171, 37, 452, 453], [454, 154, 15, 49, 455, 19, 456, 457, 96], [70, 172, 157, 458, 49, 12, 173, 28, 46, 19, 32, 459, 4, 460, 144, 4, 461, 2, 110, 7, 20, 28, 146, 174, 38, 2, 110, 7, 20, 93, 172, 26, 28], [23, 11, 26, 13, 94, 462, 62, 175, 2, 463], [41, 464, 40, 58, 465, 149, 8, 73, 3, 26, 98], [466, 6, 4, 106, 7, 176, 12, 71, 2], [21, 36, 3, 67, 467], [16, 112, 16, 112], [468, 177, 3], [135, 469, 470, 471, 472, 473, 474, 8, 85, 475, 476, 14, 10, 4, 477, 478, 479, 480, 481, 148, 482, 483, 484], [21, 178, 3], [485, 486, 487, 78, 51, 66, 488, 489, 3, 490, 491, 38, 10, 13, 492, 493, 41, 6, 2], [50, 494, 495, 19, 14, 7, 496, 43, 497, 179, 498, 499, 500, 8, 24, 501, 3], [0, 0, 0, 5, 0, 0, 6, 0, 0, 0, 0, 6, 0, 152], [0, 0], [0, 27, 0, 3, 24, 77, 63, 15, 13, 33, 0, 0, 9, 74, 12, 3, 26, 25], [82, 0, 180, 3, 181, 0, 63, 17, 167, 111, 0], [64, 28], [46, 182, 0], [0, 0, 140, 0], [48, 22, 16, 49, 109, 35, 65, 48, 0, 36, 44, 66, 0, 0, 182, 22, 0, 0], [82, 115, 0, 67, 11], [0, 27, 0, 0, 0, 0, 76, 0, 162, 0, 0, 2, 3, 9, 8, 73, 78, 23], [61, 41, 0, 2, 3, 42, 101, 14, 10, 5, 32, 17, 2], [179, 0, 0, 0], [23, 64, 8, 9, 71, 25, 0, 103, 0, 10, 83, 0, 18, 51, 5, 177, 6, 8, 0, 0], [123, 0, 0, 0, 15, 13, 131, 166, 0, 0, 171, 41, 0, 6, 4, 9, 4, 7, 49, 0, 24, 35, 40, 124, 16, 112], [181, 16, 8, 9, 4, 7, 28, 0, 53, 0, 0, 34, 66, 118, 0, 6, 4, 159, 120, 126, 53, 2, 0, 0, 15, 40, 81, 45, 99, 54, 0, 0, 10, 40, 0, 0, 0, 22, 0, 0], [21, 10, 0, 21, 178, 0, 5, 0, 35, 20, 2], [25, 0, 12, 0, 106, 10, 59], [183, 38, 183, 37, 40], [0, 15, 13, 0, 65, 0, 62, 0, 100, 55, 59, 0, 60, 143, 91, 70, 0, 55, 0, 0, 6, 2], [67, 91, 0, 31, 0, 65, 62, 19, 32, 10, 2, 0, 0, 0, 2], [51, 18, 176, 0, 0, 18, 57], [0], [0, 64, 0, 0], [180, 107, 0, 0, 0, 0, 63, 17, 0, 9, 76, 125, 0, 0, 0, 174, 0, 0, 4, 47, 8, 184, 0, 24, 77, 0, 0, 0, 0, 0, 109, 185, 52, 79, 18, 0, 5, 0, 30, 0, 44, 0, 34, 13, 52, 22, 0, 186, 0, 0, 0, 57, 52, 0, 0, 147, 2, 0, 2, 127, 34, 0], [108, 0, 165, 0, 6, 4, 0, 0], [0, 78, 51, 20, 2, 6, 35, 169, 170, 20, 2, 111], [3, 67, 0, 11, 69, 19, 14, 7, 2, 0, 72, 90, 8, 77, 44, 23, 101, 0, 5, 163, 32], [60, 0, 105, 30, 103, 130, 15, 13, 23, 12, 71, 83, 107, 50, 31, 104, 9, 4, 47, 40], [0, 9, 4, 47, 2, 0, 0, 0, 97, 63, 0], [0, 0, 43, 0, 54, 22, 0], [33, 11, 2, 11, 0, 0, 0, 0, 0], [0, 12, 39, 6, 187, 65, 185, 3, 52, 3, 70, 187, 175, 2, 0], [50, 31, 36, 3, 67, 11, 2], [0, 0, 0, 0, 142, 20, 49, 12, 173, 0, 46, 34, 13, 0], [79, 27, 9, 4, 0, 42, 15, 13, 33, 0, 3, 26, 59, 0, 37, 5, 68, 0, 4, 0, 95, 139, 0, 0, 0, 0, 34, 2, 0, 186, 98, 145, 100, 48, 66, 0, 90, 72, 3, 87, 69, 46, 168, 184, 45, 6, 0, 22, 80, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 길이가 다른 문장들을 모두 동일한 길이로 바꿔줌 -> 패딩\n",
    " - 패딩 작업이란 정해준 길이로 모든 샘플들의 길이를 맞춰주되, 길이가 정해준 길이보다 짧은 샘플들에는 'pad' 토큰을 추가하여 길이를 맞춰주는 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 62\n",
      "리뷰의 최소 길이 : 1\n",
      "리뷰의 평균 길이 : 13.900000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_len = max(len(l) for l in encoded)\n",
    "print('리뷰의 최대 길이 : %d' % max_len)\n",
    "print('리뷰의 최소 길이 : %d' % min(len(l) for l in encoded))\n",
    "print('리뷰의 평균 길이 : %f' % (sum(map(len, encoded))/len(encoded)))\n",
    "plt.hist([len(s) for s in encoded], bins=50)\n",
    "plt.xlabel('length of sample')\n",
    "plt.ylabel('number of sample')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 리뷰의 길이를 62로 맞춤\n",
    "for line in encoded:\n",
    "    if len(line) < max_len: # 현재 샘플이 정해준 길이보다 짧으면\n",
    "        line += [word_to_index['pad']] * (max_len - len(line)) # 나머지는 전부 'pad' 토큰으로 채운다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 62\n",
      "리뷰의 최소 길이 : 62\n",
      "리뷰의 평균 길이 : 62.000000\n"
     ]
    }
   ],
   "source": [
    "print('리뷰의 최대 길이 : %d' % max(len(l) for l in encoded))\n",
    "print('리뷰의 최소 길이 : %d' % min(len(l) for l in encoded))\n",
    "print('리뷰의 평균 길이 : %f' % (sum(map(len, encoded))/len(encoded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[79, 27, 9, 4, 50, 42, 80, 16, 28, 29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [188, 5, 81, 189, 190, 191, 43, 192, 113, 5, 193, 194, 24, 114, 195, 196, 13, 51, 82, 115, 30, 43, 197, 116, 117, 31, 198, 5, 199, 200, 17, 113, 7, 68, 52, 17, 44, 201, 5, 202, 4, 203, 14, 7, 83, 32, 204, 84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [205, 118, 206, 53, 207, 31, 208, 209, 54, 10, 25, 11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(encoded[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1은 마스킹 된 토큰 문장 길이수 맞추기 위해서 의미업음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
