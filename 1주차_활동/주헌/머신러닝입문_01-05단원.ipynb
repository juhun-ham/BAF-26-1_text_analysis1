{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 26-1 1주차 스터디 -1 \n",
    "### 딥러닝 파이토치 교과서의 01 ~ 05단원의 파이토치, 머신러닝 내용까지 중 코딩해보고 싶은 부분들을 \n",
    "### 교재 github 코드를 참고하여 코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02-02\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3. 4. 5. 6.]\n"
     ]
    }
   ],
   "source": [
    "t = np.array([0., 1., 2., 3., 4., 5., 6.])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파이토치 텐서 선언하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1., 2., 3.],\n",
    "                      [4., 5., 6.],\n",
    "                      [7., 8., 9.],\n",
    "                      [10., 11., 12.]\n",
    "                      ])\n",
    "print(t)                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "print(t.dim()) # 차원 수\n",
    "print(t.size()) # 각 차원의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.,  5.,  8., 11.])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "print(t[:, 1]) \n",
    "print(t[:, 1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.],\n",
      "        [ 4.,  5.],\n",
      "        [ 7.,  8.],\n",
      "        [10., 11.]])\n"
     ]
    }
   ],
   "source": [
    "print(t[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# 뷰(view) - 원소의 수를 유지하면서 텐서의 크기 변경\n",
    "t = np.array([[[0, 1, 2],\n",
    "               [3, 4, 5]],\n",
    "              [[6, 7, 8],\n",
    "               [9, 10, 11]]])\n",
    "ft = torch.FloatTensor(t)\n",
    "\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.],\n",
       "         [ 3.,  4.,  5.]],\n",
       "\n",
       "        [[ 6.,  7.,  8.],\n",
       "         [ 9., 10., 11.]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.]])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.view([-1, 3]))\n",
    "print(ft.view([-1, 3]).shape) # 2차원 텐서로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  2.]],\n",
      "\n",
      "        [[ 3.,  4.,  5.]],\n",
      "\n",
      "        [[ 6.,  7.,  8.]],\n",
      "\n",
      "        [[ 9., 10., 11.]]])\n",
      "torch.Size([4, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.view([-1, 1, 3]))\n",
    "print(ft.view([-1, 1, 3]).shape) # 3차원 텐서로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "ft = torch.FloatTensor([[0], [1], [2]])\n",
    "print(ft)\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.random.manual_seed(seed) -> torch._C.Generator>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "torch.Size([3, 1])\n",
      "----------------------\n",
      "tensor([[2.],\n",
      "        [4.],\n",
      "        [6.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "# 변수 선언\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "print(x_train)\n",
    "print(x_train.shape)\n",
    "print('----------------------')\n",
    "print(y_train)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros(1, requires_grad=True) # 가중치 초기화\n",
    "\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "b = torch.zeros(1, requires_grad=True) # 편향 초기화\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = x_train * W + b\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.6667, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([W, b], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad() # 기울기 초기화\n",
    "\n",
    "cost.backward() # 기울기 계산\n",
    "\n",
    "optimizer.step() # 가중치 갱신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 W: 0.187, b: 0.080 Cost: 18.666666\n",
      "Epoch  100/2000 W: 1.746, b: 0.578 Cost: 0.048171\n",
      "Epoch  200/2000 W: 1.800, b: 0.454 Cost: 0.029767\n",
      "Epoch  300/2000 W: 1.843, b: 0.357 Cost: 0.018394\n",
      "Epoch  400/2000 W: 1.876, b: 0.281 Cost: 0.011366\n",
      "Epoch  500/2000 W: 1.903, b: 0.221 Cost: 0.007024\n",
      "Epoch  600/2000 W: 1.924, b: 0.174 Cost: 0.004340\n",
      "Epoch  700/2000 W: 1.940, b: 0.136 Cost: 0.002682\n",
      "Epoch  800/2000 W: 1.953, b: 0.107 Cost: 0.001657\n",
      "Epoch  900/2000 W: 1.963, b: 0.084 Cost: 0.001024\n",
      "Epoch 1000/2000 W: 1.971, b: 0.066 Cost: 0.000633\n",
      "Epoch 1100/2000 W: 1.977, b: 0.052 Cost: 0.000391\n",
      "Epoch 1200/2000 W: 1.982, b: 0.041 Cost: 0.000242\n",
      "Epoch 1300/2000 W: 1.986, b: 0.032 Cost: 0.000149\n",
      "Epoch 1400/2000 W: 1.989, b: 0.025 Cost: 0.000092\n",
      "Epoch 1500/2000 W: 1.991, b: 0.020 Cost: 0.000057\n",
      "Epoch 1600/2000 W: 1.993, b: 0.016 Cost: 0.000035\n",
      "Epoch 1700/2000 W: 1.995, b: 0.012 Cost: 0.000022\n",
      "Epoch 1800/2000 W: 1.996, b: 0.010 Cost: 0.000013\n",
      "Epoch 1900/2000 W: 1.997, b: 0.008 Cost: 0.000008\n",
      "Epoch 2000/2000 W: 1.997, b: 0.006 Cost: 0.000005\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "nb_epochs = 2000 # 원하는만큼 경사 하강법을 반복\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train * W + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch    1/20 hypothesis: tensor([66.7178, 80.1701, 76.1025, 86.0194, 61.1565]) Cost: 9537.694336\n",
      "Epoch    2/20 hypothesis: tensor([104.5421, 125.6208, 119.2478, 134.7862,  95.8280]) Cost: 3069.590088\n",
      "Epoch    3/20 hypothesis: tensor([125.9858, 151.3882, 143.7087, 162.4333, 115.4844]) Cost: 990.670288\n",
      "Epoch    4/20 hypothesis: tensor([138.1429, 165.9963, 157.5768, 178.1071, 126.6283]) Cost: 322.481873\n",
      "Epoch    5/20 hypothesis: tensor([145.0350, 174.2780, 165.4395, 186.9928, 132.9461]) Cost: 107.717064\n",
      "Epoch    6/20 hypothesis: tensor([148.9423, 178.9730, 169.8976, 192.0301, 136.5279]) Cost: 38.687496\n",
      "Epoch    7/20 hypothesis: tensor([151.1574, 181.6346, 172.4254, 194.8856, 138.5585]) Cost: 16.499043\n",
      "Epoch    8/20 hypothesis: tensor([152.4131, 183.1435, 173.8590, 196.5043, 139.7097]) Cost: 9.365656\n",
      "Epoch    9/20 hypothesis: tensor([153.1250, 183.9988, 174.6723, 197.4217, 140.3625]) Cost: 7.071114\n",
      "Epoch   10/20 hypothesis: tensor([153.5285, 184.4835, 175.1338, 197.9415, 140.7325]) Cost: 6.331847\n",
      "Epoch   11/20 hypothesis: tensor([153.7572, 184.7582, 175.3958, 198.2360, 140.9424]) Cost: 6.092532\n",
      "Epoch   12/20 hypothesis: tensor([153.8868, 184.9138, 175.5449, 198.4026, 141.0613]) Cost: 6.013817\n",
      "Epoch   13/20 hypothesis: tensor([153.9602, 185.0019, 175.6299, 198.4969, 141.1288]) Cost: 5.986785\n",
      "Epoch   14/20 hypothesis: tensor([154.0017, 185.0517, 175.6785, 198.5500, 141.1671]) Cost: 5.976325\n",
      "Epoch   15/20 hypothesis: tensor([154.0252, 185.0798, 175.7065, 198.5800, 141.1888]) Cost: 5.971208\n",
      "Epoch   16/20 hypothesis: tensor([154.0385, 185.0956, 175.7229, 198.5966, 141.2012]) Cost: 5.967835\n",
      "Epoch   17/20 hypothesis: tensor([154.0459, 185.1045, 175.7326, 198.6059, 141.2082]) Cost: 5.964969\n",
      "Epoch   18/20 hypothesis: tensor([154.0501, 185.1094, 175.7386, 198.6108, 141.2122]) Cost: 5.962291\n",
      "Epoch   19/20 hypothesis: tensor([154.0524, 185.1120, 175.7424, 198.6134, 141.2145]) Cost: 5.959664\n",
      "Epoch   20/20 hypothesis: tensor([154.0536, 185.1134, 175.7451, 198.6145, 141.2158]) Cost: 5.957089\n"
     ]
    }
   ],
   "source": [
    "x_train  =  torch.FloatTensor([[73,  80,  75],\n",
    "                               [93,  88,  93],\n",
    "                               [89,  91,  80],\n",
    "                               [96,  98,  100],\n",
    "                               [73,  66,  70]])\n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros((3, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    # 편향 b는 브로드 캐스팅되어 각 샘플에 더해집니다.\n",
    "    hypothesis = x_train.matmul(W) + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value for input [75.0, 85.0, 72.0]: 156.8051300048828\n"
     ]
    }
   ],
   "source": [
    "# 임의의 입력 값에 대한 예측\n",
    "with torch.no_grad():\n",
    "    new_input = torch.FloatTensor([[75, 85, 72]])  # 예측하고 싶은 임의의 입력\n",
    "    prediction = new_input.matmul(W) + b\n",
    "    print('Predicted value for input {}: {}'.format(new_input.squeeze().tolist(), prediction.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03-03 nn.Module과 클래스로 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Module과 클래스로 구현하기\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10d054790>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(1, 1) # 입력 차원 1, 출력 차원 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.5153]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4414], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters())) # 가중치와 편향 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 13.103541\n",
      "Epoch  100/2000 Cost: 0.002791\n",
      "Epoch  200/2000 Cost: 0.001724\n",
      "Epoch  300/2000 Cost: 0.001066\n",
      "Epoch  400/2000 Cost: 0.000658\n",
      "Epoch  500/2000 Cost: 0.000407\n",
      "Epoch  600/2000 Cost: 0.000251\n",
      "Epoch  700/2000 Cost: 0.000155\n",
      "Epoch  800/2000 Cost: 0.000096\n",
      "Epoch  900/2000 Cost: 0.000059\n",
      "Epoch 1000/2000 Cost: 0.000037\n",
      "Epoch 1100/2000 Cost: 0.000023\n",
      "Epoch 1200/2000 Cost: 0.000014\n",
      "Epoch 1300/2000 Cost: 0.000009\n",
      "Epoch 1400/2000 Cost: 0.000005\n",
      "Epoch 1500/2000 Cost: 0.000003\n",
      "Epoch 1600/2000 Cost: 0.000002\n",
      "Epoch 1700/2000 Cost: 0.000001\n",
      "Epoch 1800/2000 Cost: 0.000001\n",
      "Epoch 1900/2000 Cost: 0.000000\n",
      "Epoch 2000/2000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# 전체 훈련 데이터에 대해 경사 하강법을 2,000회 반복\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "    # cost로 H(x) 개선하는 부분\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 비용 함수를 미분하여 gradient 계산\n",
    "    cost.backward() # backward 연산\n",
    "    # W와 b를 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 4일때의 예측값 :  tensor([[7.9989]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 입력 4를 선언\n",
    "new_var = torch.FloatTensor([[4.0]]) # 대괄호 2개 주의 \n",
    "# 예측 \n",
    "pred_y = model(new_var)\n",
    "\n",
    "print('훈련 후 입력이 4일때의 예측값 : ', pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[1.9994]], requires_grad=True), Parameter containing:\n",
      "tensor([0.0014], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters())) # W가 2에 가깝고, b의 값이 0에 가까움을 확인할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 다중 선형 회귀 구현하기\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10d054790>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3개의 x로부터 하나의 y를 예측하는 문제\n",
    "# 데이터\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(3, 1) # 입력 차원 3, 출력 차원 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.2975, -0.2548, -0.1119]], requires_grad=True), Parameter containing:\n",
      "tensor([0.2710], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters())) # 가중치와 편향 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 1e-05\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 31667.597656\n",
      "Epoch  100/2000 Cost: 0.225993\n",
      "Epoch  200/2000 Cost: 0.223911\n",
      "Epoch  300/2000 Cost: 0.221941\n",
      "Epoch  400/2000 Cost: 0.220059\n",
      "Epoch  500/2000 Cost: 0.218271\n",
      "Epoch  600/2000 Cost: 0.216575\n",
      "Epoch  700/2000 Cost: 0.214950\n",
      "Epoch  800/2000 Cost: 0.213413\n",
      "Epoch  900/2000 Cost: 0.211952\n",
      "Epoch 1000/2000 Cost: 0.210560\n",
      "Epoch 1100/2000 Cost: 0.209232\n",
      "Epoch 1200/2000 Cost: 0.207967\n",
      "Epoch 1300/2000 Cost: 0.206761\n",
      "Epoch 1400/2000 Cost: 0.205619\n",
      "Epoch 1500/2000 Cost: 0.204522\n",
      "Epoch 1600/2000 Cost: 0.203484\n",
      "Epoch 1700/2000 Cost: 0.202485\n",
      "Epoch 1800/2000 Cost: 0.201542\n",
      "Epoch 1900/2000 Cost: 0.200635\n",
      "Epoch 2000/2000 Cost: 0.199769\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    # model(x_train)은 model.forward(x_train)와 동일함.\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "    # cost로 H(x) 개선하는 부분\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 비용 함수를 미분하여 gradient 계산\n",
    "    cost.backward()\n",
    "    # W와 b를 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[151.2305]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 입력 [73, 80, 75]를 선언\n",
    "new_var =  torch.FloatTensor([[73, 80, 75]])\n",
    "# 예측\n",
    "pred_y = model(new_var)\n",
    "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.9778, 0.4539, 0.5768]], requires_grad=True), Parameter containing:\n",
      "tensor([0.2802], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 모델을 클래스로 구현하기\n",
    "# 단순 선형 회귀 모델 클래스로 구현하기\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1,1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 15.171722\n",
      "Epoch  100/2000 Cost: 0.006754\n",
      "Epoch  200/2000 Cost: 0.004174\n",
      "Epoch  300/2000 Cost: 0.002579\n",
      "Epoch  400/2000 Cost: 0.001594\n",
      "Epoch  500/2000 Cost: 0.000985\n",
      "Epoch  600/2000 Cost: 0.000609\n",
      "Epoch  700/2000 Cost: 0.000376\n",
      "Epoch  800/2000 Cost: 0.000232\n",
      "Epoch  900/2000 Cost: 0.000144\n",
      "Epoch 1000/2000 Cost: 0.000089\n",
      "Epoch 1100/2000 Cost: 0.000055\n",
      "Epoch 1200/2000 Cost: 0.000034\n",
      "Epoch 1300/2000 Cost: 0.000021\n",
      "Epoch 1400/2000 Cost: 0.000013\n",
      "Epoch 1500/2000 Cost: 0.000008\n",
      "Epoch 1600/2000 Cost: 0.000005\n",
      "Epoch 1700/2000 Cost: 0.000003\n",
      "Epoch 1800/2000 Cost: 0.000002\n",
      "Epoch 1900/2000 Cost: 0.000001\n",
      "Epoch 2000/2000 Cost: 0.000001\n"
     ]
    }
   ],
   "source": [
    "# 전체 훈련 데이터에 대해 경사 하강법을 2,000회 반복\n",
    "nb_epochs = 2000\n",
    "\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "    # cost로 H(x) 개선하는 부분\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 비용 함수를 미분하여 gradient 계산\n",
    "    cost.backward() # backward 연산\n",
    "    # W와 b를 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다중 회귀 모델 클래스\n",
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3,1)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultivariateLinearRegressionModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03-04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. DataLoader로 데이터 다루기\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train  =  torch.FloatTensor([[73,  80,  75],\n",
    "                               [93,  88,  93],\n",
    "                               [89,  91,  90],\n",
    "                               [96,  98,  100],\n",
    "                               [73,  66,  70]])\n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(3,1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[tensor([[73., 66., 70.],\n",
      "        [73., 80., 75.]]), tensor([[142.],\n",
      "        [152.]])]\n",
      "Epoch    0/20 Batch 1/3 Cost: 2421.056641\n",
      "1\n",
      "[tensor([[89., 91., 90.],\n",
      "        [93., 88., 93.]]), tensor([[180.],\n",
      "        [185.]])]\n",
      "Epoch    0/20 Batch 2/3 Cost: 1692.770874\n",
      "2\n",
      "[tensor([[ 96.,  98., 100.]]), tensor([[196.]])]\n",
      "Epoch    0/20 Batch 3/3 Cost: 441.154449\n",
      "0\n",
      "[tensor([[ 96.,  98., 100.],\n",
      "        [ 73.,  66.,  70.]]), tensor([[196.],\n",
      "        [142.]])]\n",
      "Epoch    1/20 Batch 1/3 Cost: 83.005875\n",
      "1\n",
      "[tensor([[73., 80., 75.],\n",
      "        [89., 91., 90.]]), tensor([[152.],\n",
      "        [180.]])]\n",
      "Epoch    1/20 Batch 2/3 Cost: 16.265928\n",
      "2\n",
      "[tensor([[93., 88., 93.]]), tensor([[185.]])]\n",
      "Epoch    1/20 Batch 3/3 Cost: 23.012938\n",
      "0\n",
      "[tensor([[ 89.,  91.,  90.],\n",
      "        [ 96.,  98., 100.]]), tensor([[180.],\n",
      "        [196.]])]\n",
      "Epoch    2/20 Batch 1/3 Cost: 0.022598\n",
      "1\n",
      "[tensor([[73., 66., 70.],\n",
      "        [73., 80., 75.]]), tensor([[142.],\n",
      "        [152.]])]\n",
      "Epoch    2/20 Batch 2/3 Cost: 4.206723\n",
      "2\n",
      "[tensor([[93., 88., 93.]]), tensor([[185.]])]\n",
      "Epoch    2/20 Batch 3/3 Cost: 3.245286\n",
      "0\n",
      "[tensor([[ 96.,  98., 100.],\n",
      "        [ 93.,  88.,  93.]]), tensor([[196.],\n",
      "        [185.]])]\n",
      "Epoch    3/20 Batch 1/3 Cost: 1.902188\n",
      "1\n",
      "[tensor([[73., 66., 70.],\n",
      "        [89., 91., 90.]]), tensor([[142.],\n",
      "        [180.]])]\n",
      "Epoch    3/20 Batch 2/3 Cost: 2.457229\n",
      "2\n",
      "[tensor([[73., 80., 75.]]), tensor([[152.]])]\n",
      "Epoch    3/20 Batch 3/3 Cost: 0.944547\n",
      "0\n",
      "[tensor([[73., 66., 70.],\n",
      "        [89., 91., 90.]]), tensor([[142.],\n",
      "        [180.]])]\n",
      "Epoch    4/20 Batch 1/3 Cost: 2.651322\n",
      "1\n",
      "[tensor([[73., 80., 75.],\n",
      "        [93., 88., 93.]]), tensor([[152.],\n",
      "        [185.]])]\n",
      "Epoch    4/20 Batch 2/3 Cost: 1.069000\n",
      "2\n",
      "[tensor([[ 96.,  98., 100.]]), tensor([[196.]])]\n",
      "Epoch    4/20 Batch 3/3 Cost: 2.360735\n",
      "0\n",
      "[tensor([[73., 66., 70.],\n",
      "        [89., 91., 90.]]), tensor([[142.],\n",
      "        [180.]])]\n",
      "Epoch    5/20 Batch 1/3 Cost: 3.238422\n",
      "1\n",
      "[tensor([[ 96.,  98., 100.],\n",
      "        [ 93.,  88.,  93.]]), tensor([[196.],\n",
      "        [185.]])]\n",
      "Epoch    5/20 Batch 2/3 Cost: 1.710319\n",
      "2\n",
      "[tensor([[73., 80., 75.]]), tensor([[152.]])]\n",
      "Epoch    5/20 Batch 3/3 Cost: 0.420372\n",
      "0\n",
      "[tensor([[73., 80., 75.],\n",
      "        [89., 91., 90.]]), tensor([[152.],\n",
      "        [180.]])]\n",
      "Epoch    6/20 Batch 1/3 Cost: 0.227619\n",
      "1\n",
      "[tensor([[73., 66., 70.],\n",
      "        [93., 88., 93.]]), tensor([[142.],\n",
      "        [185.]])]\n",
      "Epoch    6/20 Batch 2/3 Cost: 4.992033\n",
      "2\n",
      "[tensor([[ 96.,  98., 100.]]), tensor([[196.]])]\n",
      "Epoch    6/20 Batch 3/3 Cost: 2.818634\n",
      "0\n",
      "[tensor([[73., 80., 75.],\n",
      "        [73., 66., 70.]]), tensor([[152.],\n",
      "        [142.]])]\n",
      "Epoch    7/20 Batch 1/3 Cost: 3.116337\n",
      "1\n",
      "[tensor([[ 96.,  98., 100.],\n",
      "        [ 89.,  91.,  90.]]), tensor([[196.],\n",
      "        [180.]])]\n",
      "Epoch    7/20 Batch 2/3 Cost: 0.965390\n",
      "2\n",
      "[tensor([[93., 88., 93.]]), tensor([[185.]])]\n",
      "Epoch    7/20 Batch 3/3 Cost: 3.728996\n",
      "0\n",
      "[tensor([[93., 88., 93.],\n",
      "        [73., 66., 70.]]), tensor([[185.],\n",
      "        [142.]])]\n",
      "Epoch    8/20 Batch 1/3 Cost: 2.098232\n",
      "1\n",
      "[tensor([[89., 91., 90.],\n",
      "        [73., 80., 75.]]), tensor([[180.],\n",
      "        [152.]])]\n",
      "Epoch    8/20 Batch 2/3 Cost: 2.769015\n",
      "2\n",
      "[tensor([[ 96.,  98., 100.]]), tensor([[196.]])]\n",
      "Epoch    8/20 Batch 3/3 Cost: 2.156061\n",
      "0\n",
      "[tensor([[73., 80., 75.],\n",
      "        [89., 91., 90.]]), tensor([[152.],\n",
      "        [180.]])]\n",
      "Epoch    9/20 Batch 1/3 Cost: 0.063504\n",
      "1\n",
      "[tensor([[ 96.,  98., 100.],\n",
      "        [ 93.,  88.,  93.]]), tensor([[196.],\n",
      "        [185.]])]\n",
      "Epoch    9/20 Batch 2/3 Cost: 2.204478\n",
      "2\n",
      "[tensor([[73., 66., 70.]]), tensor([[142.]])]\n",
      "Epoch    9/20 Batch 3/3 Cost: 5.475368\n",
      "0\n",
      "[tensor([[ 96.,  98., 100.],\n",
      "        [ 73.,  66.,  70.]]), tensor([[196.],\n",
      "        [142.]])]\n",
      "Epoch   10/20 Batch 1/3 Cost: 3.104992\n",
      "1\n",
      "[tensor([[89., 91., 90.],\n",
      "        [73., 80., 75.]]), tensor([[180.],\n",
      "        [152.]])]\n",
      "Epoch   10/20 Batch 2/3 Cost: 1.291868\n",
      "2\n",
      "[tensor([[93., 88., 93.]]), tensor([[185.]])]\n",
      "Epoch   10/20 Batch 3/3 Cost: 2.161307\n",
      "0\n",
      "[tensor([[ 73.,  80.,  75.],\n",
      "        [ 96.,  98., 100.]]), tensor([[152.],\n",
      "        [196.]])]\n",
      "Epoch   11/20 Batch 1/3 Cost: 2.521379\n",
      "1\n",
      "[tensor([[93., 88., 93.],\n",
      "        [89., 91., 90.]]), tensor([[185.],\n",
      "        [180.]])]\n",
      "Epoch   11/20 Batch 2/3 Cost: 1.363634\n",
      "2\n",
      "[tensor([[73., 66., 70.]]), tensor([[142.]])]\n",
      "Epoch   11/20 Batch 3/3 Cost: 4.229520\n",
      "0\n",
      "[tensor([[ 96.,  98., 100.],\n",
      "        [ 73.,  80.,  75.]]), tensor([[196.],\n",
      "        [152.]])]\n",
      "Epoch   12/20 Batch 1/3 Cost: 3.248589\n",
      "1\n",
      "[tensor([[93., 88., 93.],\n",
      "        [73., 66., 70.]]), tensor([[185.],\n",
      "        [142.]])]\n",
      "Epoch   12/20 Batch 2/3 Cost: 3.174246\n",
      "2\n",
      "[tensor([[89., 91., 90.]]), tensor([[180.]])]\n",
      "Epoch   12/20 Batch 3/3 Cost: 2.434545\n",
      "0\n",
      "[tensor([[73., 80., 75.],\n",
      "        [89., 91., 90.]]), tensor([[152.],\n",
      "        [180.]])]\n",
      "Epoch   13/20 Batch 1/3 Cost: 0.532021\n",
      "1\n",
      "[tensor([[ 93.,  88.,  93.],\n",
      "        [ 96.,  98., 100.]]), tensor([[185.],\n",
      "        [196.]])]\n",
      "Epoch   13/20 Batch 2/3 Cost: 1.832024\n",
      "2\n",
      "[tensor([[73., 66., 70.]]), tensor([[142.]])]\n",
      "Epoch   13/20 Batch 3/3 Cost: 4.950924\n",
      "0\n",
      "[tensor([[93., 88., 93.],\n",
      "        [89., 91., 90.]]), tensor([[185.],\n",
      "        [180.]])]\n",
      "Epoch   14/20 Batch 1/3 Cost: 1.371169\n",
      "1\n",
      "[tensor([[ 73.,  80.,  75.],\n",
      "        [ 96.,  98., 100.]]), tensor([[152.],\n",
      "        [196.]])]\n",
      "Epoch   14/20 Batch 2/3 Cost: 2.099151\n",
      "2\n",
      "[tensor([[73., 66., 70.]]), tensor([[142.]])]\n",
      "Epoch   14/20 Batch 3/3 Cost: 5.115372\n",
      "0\n",
      "[tensor([[ 93.,  88.,  93.],\n",
      "        [ 96.,  98., 100.]]), tensor([[185.],\n",
      "        [196.]])]\n",
      "Epoch   15/20 Batch 1/3 Cost: 2.099269\n",
      "1\n",
      "[tensor([[73., 80., 75.],\n",
      "        [73., 66., 70.]]), tensor([[152.],\n",
      "        [142.]])]\n",
      "Epoch   15/20 Batch 2/3 Cost: 2.170743\n",
      "2\n",
      "[tensor([[89., 91., 90.]]), tensor([[180.]])]\n",
      "Epoch   15/20 Batch 3/3 Cost: 1.694414\n",
      "0\n",
      "[tensor([[73., 80., 75.],\n",
      "        [89., 91., 90.]]), tensor([[152.],\n",
      "        [180.]])]\n",
      "Epoch   16/20 Batch 1/3 Cost: 0.365929\n",
      "1\n",
      "[tensor([[ 96.,  98., 100.],\n",
      "        [ 73.,  66.,  70.]]), tensor([[196.],\n",
      "        [142.]])]\n",
      "Epoch   16/20 Batch 2/3 Cost: 3.252268\n",
      "2\n",
      "[tensor([[93., 88., 93.]]), tensor([[185.]])]\n",
      "Epoch   16/20 Batch 3/3 Cost: 2.349636\n",
      "0\n",
      "[tensor([[73., 66., 70.],\n",
      "        [73., 80., 75.]]), tensor([[142.],\n",
      "        [152.]])]\n",
      "Epoch   17/20 Batch 1/3 Cost: 2.035559\n",
      "1\n",
      "[tensor([[89., 91., 90.],\n",
      "        [93., 88., 93.]]), tensor([[180.],\n",
      "        [185.]])]\n",
      "Epoch   17/20 Batch 2/3 Cost: 1.342984\n",
      "2\n",
      "[tensor([[ 96.,  98., 100.]]), tensor([[196.]])]\n",
      "Epoch   17/20 Batch 3/3 Cost: 2.974014\n",
      "0\n",
      "[tensor([[93., 88., 93.],\n",
      "        [73., 66., 70.]]), tensor([[185.],\n",
      "        [142.]])]\n",
      "Epoch   18/20 Batch 1/3 Cost: 4.633673\n",
      "1\n",
      "[tensor([[89., 91., 90.],\n",
      "        [73., 80., 75.]]), tensor([[180.],\n",
      "        [152.]])]\n",
      "Epoch   18/20 Batch 2/3 Cost: 1.386611\n",
      "2\n",
      "[tensor([[ 96.,  98., 100.]]), tensor([[196.]])]\n",
      "Epoch   18/20 Batch 3/3 Cost: 1.302035\n",
      "0\n",
      "[tensor([[89., 91., 90.],\n",
      "        [73., 66., 70.]]), tensor([[180.],\n",
      "        [142.]])]\n",
      "Epoch   19/20 Batch 1/3 Cost: 3.447199\n",
      "1\n",
      "[tensor([[ 73.,  80.,  75.],\n",
      "        [ 96.,  98., 100.]]), tensor([[152.],\n",
      "        [196.]])]\n",
      "Epoch   19/20 Batch 2/3 Cost: 0.594501\n",
      "2\n",
      "[tensor([[93., 88., 93.]]), tensor([[185.]])]\n",
      "Epoch   19/20 Batch 3/3 Cost: 3.763669\n",
      "0\n",
      "[tensor([[73., 66., 70.],\n",
      "        [89., 91., 90.]]), tensor([[142.],\n",
      "        [180.]])]\n",
      "Epoch   20/20 Batch 1/3 Cost: 2.357485\n",
      "1\n",
      "[tensor([[ 96.,  98., 100.],\n",
      "        [ 73.,  80.,  75.]]), tensor([[196.],\n",
      "        [152.]])]\n",
      "Epoch   20/20 Batch 2/3 Cost: 1.935424\n",
      "2\n",
      "[tensor([[93., 88., 93.]]), tensor([[185.]])]\n",
      "Epoch   20/20 Batch 3/3 Cost: 2.515875\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "  for batch_idx, samples in enumerate(dataloader):\n",
    "    print(batch_idx)\n",
    "    print(samples)\n",
    "    x_train, y_train = samples\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 계산\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, batch_idx+1, len(dataloader),\n",
    "        cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[153.1300]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 입력 [73, 80, 75]를 선언\n",
    "new_var =  torch.FloatTensor([[73, 80, 75]])\n",
    "# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
    "pred_y = model(new_var)\n",
    "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 커스텀 데이터셋 만들기\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 상속\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self):\n",
    "    self.x_data = [[73, 80, 75],\n",
    "                   [93, 88, 93],\n",
    "                   [89, 91, 90],\n",
    "                   [96, 98, 100],\n",
    "                   [73, 66, 70]]\n",
    "    self.y_data = [[152], [185], [180], [196], [142]]\n",
    "\n",
    "  # 총 데이터의 개수를 리턴\n",
    "  def __len__(self):\n",
    "    return len(self.x_data)\n",
    "\n",
    "  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n",
    "  def __getitem__(self, idx):\n",
    "    x = torch.FloatTensor(self.x_data[idx])\n",
    "    y = torch.FloatTensor(self.y_data[idx])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(3,1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Batch 1/3 Cost: 33551.726562\n",
      "Epoch    0/20 Batch 2/3 Cost: 13629.757812\n",
      "Epoch    0/20 Batch 3/3 Cost: 1762.106079\n",
      "Epoch    1/20 Batch 1/3 Cost: 1155.497314\n",
      "Epoch    1/20 Batch 2/3 Cost: 473.134033\n",
      "Epoch    1/20 Batch 3/3 Cost: 144.862976\n",
      "Epoch    2/20 Batch 1/3 Cost: 39.522057\n",
      "Epoch    2/20 Batch 2/3 Cost: 10.803387\n",
      "Epoch    2/20 Batch 3/3 Cost: 4.258313\n",
      "Epoch    3/20 Batch 1/3 Cost: 1.517120\n",
      "Epoch    3/20 Batch 2/3 Cost: 2.851507\n",
      "Epoch    3/20 Batch 3/3 Cost: 0.014653\n",
      "Epoch    4/20 Batch 1/3 Cost: 0.464104\n",
      "Epoch    4/20 Batch 2/3 Cost: 2.174437\n",
      "Epoch    4/20 Batch 3/3 Cost: 0.003075\n",
      "Epoch    5/20 Batch 1/3 Cost: 0.000671\n",
      "Epoch    5/20 Batch 2/3 Cost: 1.670938\n",
      "Epoch    5/20 Batch 3/3 Cost: 3.498537\n",
      "Epoch    6/20 Batch 1/3 Cost: 0.432111\n",
      "Epoch    6/20 Batch 2/3 Cost: 3.085349\n",
      "Epoch    6/20 Batch 3/3 Cost: 0.012127\n",
      "Epoch    7/20 Batch 1/3 Cost: 1.203535\n",
      "Epoch    7/20 Batch 2/3 Cost: 1.662590\n",
      "Epoch    7/20 Batch 3/3 Cost: 0.000731\n",
      "Epoch    8/20 Batch 1/3 Cost: 0.000240\n",
      "Epoch    8/20 Batch 2/3 Cost: 1.680925\n",
      "Epoch    8/20 Batch 3/3 Cost: 3.482686\n",
      "Epoch    9/20 Batch 1/3 Cost: 2.131553\n",
      "Epoch    9/20 Batch 2/3 Cost: 0.385167\n",
      "Epoch    9/20 Batch 3/3 Cost: 2.360782\n",
      "Epoch   10/20 Batch 1/3 Cost: 1.152781\n",
      "Epoch   10/20 Batch 2/3 Cost: 0.625572\n",
      "Epoch   10/20 Batch 3/3 Cost: 3.884814\n",
      "Epoch   11/20 Batch 1/3 Cost: 2.302152\n",
      "Epoch   11/20 Batch 2/3 Cost: 0.317525\n",
      "Epoch   11/20 Batch 3/3 Cost: 0.082064\n",
      "Epoch   12/20 Batch 1/3 Cost: 0.334833\n",
      "Epoch   12/20 Batch 2/3 Cost: 0.987220\n",
      "Epoch   12/20 Batch 3/3 Cost: 3.364674\n",
      "Epoch   13/20 Batch 1/3 Cost: 2.156330\n",
      "Epoch   13/20 Batch 2/3 Cost: 1.316443\n",
      "Epoch   13/20 Batch 3/3 Cost: 0.023278\n",
      "Epoch   14/20 Batch 1/3 Cost: 1.858389\n",
      "Epoch   14/20 Batch 2/3 Cost: 1.723399\n",
      "Epoch   14/20 Batch 3/3 Cost: 0.019544\n",
      "Epoch   15/20 Batch 1/3 Cost: 0.004890\n",
      "Epoch   15/20 Batch 2/3 Cost: 1.258976\n",
      "Epoch   15/20 Batch 3/3 Cost: 3.109032\n",
      "Epoch   16/20 Batch 1/3 Cost: 0.791050\n",
      "Epoch   16/20 Batch 2/3 Cost: 2.902347\n",
      "Epoch   16/20 Batch 3/3 Cost: 0.003119\n",
      "Epoch   17/20 Batch 1/3 Cost: 1.238607\n",
      "Epoch   17/20 Batch 2/3 Cost: 0.269964\n",
      "Epoch   17/20 Batch 3/3 Cost: 2.997903\n",
      "Epoch   18/20 Batch 1/3 Cost: 2.979048\n",
      "Epoch   18/20 Batch 2/3 Cost: 0.018706\n",
      "Epoch   18/20 Batch 3/3 Cost: 1.932523\n",
      "Epoch   19/20 Batch 1/3 Cost: 2.560624\n",
      "Epoch   19/20 Batch 2/3 Cost: 0.786860\n",
      "Epoch   19/20 Batch 3/3 Cost: 1.452912\n",
      "Epoch   20/20 Batch 1/3 Cost: 1.173491\n",
      "Epoch   20/20 Batch 2/3 Cost: 1.678210\n",
      "Epoch   20/20 Batch 3/3 Cost: 0.001320\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "  for batch_idx, samples in enumerate(dataloader):\n",
    "    # print(batch_idx)\n",
    "    # print(samples)\n",
    "    x_train, y_train = samples\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 계산\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, batch_idx+1, len(dataloader),\n",
    "        cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[150.4187]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 입력 [73, 80, 75]를 선언\n",
    "new_var =  torch.FloatTensor([[73, 80, 75]])\n",
    "# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
    "pred_y = model(new_var)\n",
    "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. [ML 입문] - 로지스틱 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치로 로지스틱 회귀 구현하기\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10d054790>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 2])\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros((2,1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W) + b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<MulBackward0>)\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis) # 예측값인 H(x) 출력\n",
    "print(hypothesis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = torch.sigmoid(x_train.matmul(W) + b) # 시그모이드 함수 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931]], grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "losses = -(y_train * torch.log(hypothesis) +\n",
    "           (1 - y_train) * torch.log(1 - hypothesis))\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6931, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = losses.mean()\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6931, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.binary_cross_entropy(hypothesis, y_train) # 파이토치에서 로지스틱 회귀의 비용 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 훈련과정까지 추가한 전체 코드\n",
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 0.693147\n",
      "Epoch  100/1000 Cost: 0.134722\n",
      "Epoch  200/1000 Cost: 0.080643\n",
      "Epoch  300/1000 Cost: 0.057900\n",
      "Epoch  400/1000 Cost: 0.045300\n",
      "Epoch  500/1000 Cost: 0.037261\n",
      "Epoch  600/1000 Cost: 0.031673\n",
      "Epoch  700/1000 Cost: 0.027556\n",
      "Epoch  800/1000 Cost: 0.024394\n",
      "Epoch  900/1000 Cost: 0.021888\n",
      "Epoch 1000/1000 Cost: 0.019852\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "W = torch.zeros((2, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # Cost 계산\n",
    "    hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
    "    cost = -(y_train * torch.log(hypothesis) +\n",
    "             (1 - y_train) * torch.log(1 - hypothesis)).mean()\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.7648e-04],\n",
      "        [3.1608e-02],\n",
      "        [3.8977e-02],\n",
      "        [9.5622e-01],\n",
      "        [9.9823e-01],\n",
      "        [9.9969e-01]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True]])\n"
     ]
    }
   ],
   "source": [
    "prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.2530],\n",
      "        [1.5179]], requires_grad=True)\n",
      "tensor([-14.4819], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04-02 nn.Module과 클래스로 구현하는 로지스틱 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10d054790>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 1),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4020],\n",
       "        [0.4147],\n",
       "        [0.6556],\n",
       "        [0.5948],\n",
       "        [0.6788],\n",
       "        [0.8061]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 0.539713 Accuracy 83.33%\n",
      "Epoch   10/1000 Cost: 0.614851 Accuracy 66.67%\n",
      "Epoch   20/1000 Cost: 0.441875 Accuracy 66.67%\n",
      "Epoch   30/1000 Cost: 0.373145 Accuracy 83.33%\n",
      "Epoch   40/1000 Cost: 0.316358 Accuracy 83.33%\n",
      "Epoch   50/1000 Cost: 0.266094 Accuracy 83.33%\n",
      "Epoch   60/1000 Cost: 0.220498 Accuracy 100.00%\n",
      "Epoch   70/1000 Cost: 0.182095 Accuracy 100.00%\n",
      "Epoch   80/1000 Cost: 0.157299 Accuracy 100.00%\n",
      "Epoch   90/1000 Cost: 0.144091 Accuracy 100.00%\n",
      "Epoch  100/1000 Cost: 0.134272 Accuracy 100.00%\n",
      "Epoch  110/1000 Cost: 0.125769 Accuracy 100.00%\n",
      "Epoch  120/1000 Cost: 0.118297 Accuracy 100.00%\n",
      "Epoch  130/1000 Cost: 0.111680 Accuracy 100.00%\n",
      "Epoch  140/1000 Cost: 0.105779 Accuracy 100.00%\n",
      "Epoch  150/1000 Cost: 0.100483 Accuracy 100.00%\n",
      "Epoch  160/1000 Cost: 0.095704 Accuracy 100.00%\n",
      "Epoch  170/1000 Cost: 0.091369 Accuracy 100.00%\n",
      "Epoch  180/1000 Cost: 0.087420 Accuracy 100.00%\n",
      "Epoch  190/1000 Cost: 0.083806 Accuracy 100.00%\n",
      "Epoch  200/1000 Cost: 0.080486 Accuracy 100.00%\n",
      "Epoch  210/1000 Cost: 0.077425 Accuracy 100.00%\n",
      "Epoch  220/1000 Cost: 0.074595 Accuracy 100.00%\n",
      "Epoch  230/1000 Cost: 0.071969 Accuracy 100.00%\n",
      "Epoch  240/1000 Cost: 0.069526 Accuracy 100.00%\n",
      "Epoch  250/1000 Cost: 0.067248 Accuracy 100.00%\n",
      "Epoch  260/1000 Cost: 0.065118 Accuracy 100.00%\n",
      "Epoch  270/1000 Cost: 0.063122 Accuracy 100.00%\n",
      "Epoch  280/1000 Cost: 0.061247 Accuracy 100.00%\n",
      "Epoch  290/1000 Cost: 0.059483 Accuracy 100.00%\n",
      "Epoch  300/1000 Cost: 0.057820 Accuracy 100.00%\n",
      "Epoch  310/1000 Cost: 0.056250 Accuracy 100.00%\n",
      "Epoch  320/1000 Cost: 0.054764 Accuracy 100.00%\n",
      "Epoch  330/1000 Cost: 0.053357 Accuracy 100.00%\n",
      "Epoch  340/1000 Cost: 0.052022 Accuracy 100.00%\n",
      "Epoch  350/1000 Cost: 0.050753 Accuracy 100.00%\n",
      "Epoch  360/1000 Cost: 0.049546 Accuracy 100.00%\n",
      "Epoch  370/1000 Cost: 0.048396 Accuracy 100.00%\n",
      "Epoch  380/1000 Cost: 0.047299 Accuracy 100.00%\n",
      "Epoch  390/1000 Cost: 0.046252 Accuracy 100.00%\n",
      "Epoch  400/1000 Cost: 0.045251 Accuracy 100.00%\n",
      "Epoch  410/1000 Cost: 0.044294 Accuracy 100.00%\n",
      "Epoch  420/1000 Cost: 0.043376 Accuracy 100.00%\n",
      "Epoch  430/1000 Cost: 0.042497 Accuracy 100.00%\n",
      "Epoch  440/1000 Cost: 0.041653 Accuracy 100.00%\n",
      "Epoch  450/1000 Cost: 0.040843 Accuracy 100.00%\n",
      "Epoch  460/1000 Cost: 0.040064 Accuracy 100.00%\n",
      "Epoch  470/1000 Cost: 0.039315 Accuracy 100.00%\n",
      "Epoch  480/1000 Cost: 0.038593 Accuracy 100.00%\n",
      "Epoch  490/1000 Cost: 0.037898 Accuracy 100.00%\n",
      "Epoch  500/1000 Cost: 0.037228 Accuracy 100.00%\n",
      "Epoch  510/1000 Cost: 0.036582 Accuracy 100.00%\n",
      "Epoch  520/1000 Cost: 0.035958 Accuracy 100.00%\n",
      "Epoch  530/1000 Cost: 0.035356 Accuracy 100.00%\n",
      "Epoch  540/1000 Cost: 0.034773 Accuracy 100.00%\n",
      "Epoch  550/1000 Cost: 0.034210 Accuracy 100.00%\n",
      "Epoch  560/1000 Cost: 0.033664 Accuracy 100.00%\n",
      "Epoch  570/1000 Cost: 0.033137 Accuracy 100.00%\n",
      "Epoch  580/1000 Cost: 0.032625 Accuracy 100.00%\n",
      "Epoch  590/1000 Cost: 0.032130 Accuracy 100.00%\n",
      "Epoch  600/1000 Cost: 0.031649 Accuracy 100.00%\n",
      "Epoch  610/1000 Cost: 0.031183 Accuracy 100.00%\n",
      "Epoch  620/1000 Cost: 0.030730 Accuracy 100.00%\n",
      "Epoch  630/1000 Cost: 0.030291 Accuracy 100.00%\n",
      "Epoch  640/1000 Cost: 0.029864 Accuracy 100.00%\n",
      "Epoch  650/1000 Cost: 0.029449 Accuracy 100.00%\n",
      "Epoch  660/1000 Cost: 0.029046 Accuracy 100.00%\n",
      "Epoch  670/1000 Cost: 0.028654 Accuracy 100.00%\n",
      "Epoch  680/1000 Cost: 0.028272 Accuracy 100.00%\n",
      "Epoch  690/1000 Cost: 0.027900 Accuracy 100.00%\n",
      "Epoch  700/1000 Cost: 0.027538 Accuracy 100.00%\n",
      "Epoch  710/1000 Cost: 0.027186 Accuracy 100.00%\n",
      "Epoch  720/1000 Cost: 0.026842 Accuracy 100.00%\n",
      "Epoch  730/1000 Cost: 0.026507 Accuracy 100.00%\n",
      "Epoch  740/1000 Cost: 0.026181 Accuracy 100.00%\n",
      "Epoch  750/1000 Cost: 0.025862 Accuracy 100.00%\n",
      "Epoch  760/1000 Cost: 0.025552 Accuracy 100.00%\n",
      "Epoch  770/1000 Cost: 0.025248 Accuracy 100.00%\n",
      "Epoch  780/1000 Cost: 0.024952 Accuracy 100.00%\n",
      "Epoch  790/1000 Cost: 0.024663 Accuracy 100.00%\n",
      "Epoch  800/1000 Cost: 0.024381 Accuracy 100.00%\n",
      "Epoch  810/1000 Cost: 0.024104 Accuracy 100.00%\n",
      "Epoch  820/1000 Cost: 0.023835 Accuracy 100.00%\n",
      "Epoch  830/1000 Cost: 0.023571 Accuracy 100.00%\n",
      "Epoch  840/1000 Cost: 0.023313 Accuracy 100.00%\n",
      "Epoch  850/1000 Cost: 0.023061 Accuracy 100.00%\n",
      "Epoch  860/1000 Cost: 0.022814 Accuracy 100.00%\n",
      "Epoch  870/1000 Cost: 0.022572 Accuracy 100.00%\n",
      "Epoch  880/1000 Cost: 0.022336 Accuracy 100.00%\n",
      "Epoch  890/1000 Cost: 0.022104 Accuracy 100.00%\n",
      "Epoch  900/1000 Cost: 0.021877 Accuracy 100.00%\n",
      "Epoch  910/1000 Cost: 0.021655 Accuracy 100.00%\n",
      "Epoch  920/1000 Cost: 0.021437 Accuracy 100.00%\n",
      "Epoch  930/1000 Cost: 0.021224 Accuracy 100.00%\n",
      "Epoch  940/1000 Cost: 0.021015 Accuracy 100.00%\n",
      "Epoch  950/1000 Cost: 0.020810 Accuracy 100.00%\n",
      "Epoch  960/1000 Cost: 0.020609 Accuracy 100.00%\n",
      "Epoch  970/1000 Cost: 0.020412 Accuracy 100.00%\n",
      "Epoch  980/1000 Cost: 0.020219 Accuracy 100.00%\n",
      "Epoch  990/1000 Cost: 0.020029 Accuracy 100.00%\n",
      "Epoch 1000/1000 Cost: 0.019843 Accuracy 100.00%\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 10 == 0:\n",
    "        prediction = hypothesis >= torch.FloatTensor([0.5]) # 예측값이 0.5를 넘으면 True로 간주\n",
    "        correct_prediction = prediction.float() == y_train # 실제값과 일치하는 경우만 True로 간주\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도를 계산\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format( # 각 에포크마다 정확도를 출력\n",
    "            epoch, nb_epochs, cost.item(), accuracy * 100,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7616e-04],\n",
       "        [3.1595e-02],\n",
       "        [3.8959e-02],\n",
       "        [9.5624e-01],\n",
       "        [9.9823e-01],\n",
       "        [9.9969e-01]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[3.2534, 1.5181]], requires_grad=True), Parameter containing:\n",
      "tensor([-14.4839], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스로 로지스틱 회귀 모델 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10d054790>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))\n",
    "    \n",
    "model = BinaryClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 0.539713 Accuracy 83.33%\n",
      "Epoch   10/1000 Cost: 0.614851 Accuracy 66.67%\n",
      "Epoch   20/1000 Cost: 0.441875 Accuracy 66.67%\n",
      "Epoch   30/1000 Cost: 0.373145 Accuracy 83.33%\n",
      "Epoch   40/1000 Cost: 0.316358 Accuracy 83.33%\n",
      "Epoch   50/1000 Cost: 0.266094 Accuracy 83.33%\n",
      "Epoch   60/1000 Cost: 0.220498 Accuracy 100.00%\n",
      "Epoch   70/1000 Cost: 0.182095 Accuracy 100.00%\n",
      "Epoch   80/1000 Cost: 0.157299 Accuracy 100.00%\n",
      "Epoch   90/1000 Cost: 0.144091 Accuracy 100.00%\n",
      "Epoch  100/1000 Cost: 0.134272 Accuracy 100.00%\n",
      "Epoch  110/1000 Cost: 0.125769 Accuracy 100.00%\n",
      "Epoch  120/1000 Cost: 0.118297 Accuracy 100.00%\n",
      "Epoch  130/1000 Cost: 0.111680 Accuracy 100.00%\n",
      "Epoch  140/1000 Cost: 0.105779 Accuracy 100.00%\n",
      "Epoch  150/1000 Cost: 0.100483 Accuracy 100.00%\n",
      "Epoch  160/1000 Cost: 0.095704 Accuracy 100.00%\n",
      "Epoch  170/1000 Cost: 0.091369 Accuracy 100.00%\n",
      "Epoch  180/1000 Cost: 0.087420 Accuracy 100.00%\n",
      "Epoch  190/1000 Cost: 0.083806 Accuracy 100.00%\n",
      "Epoch  200/1000 Cost: 0.080486 Accuracy 100.00%\n",
      "Epoch  210/1000 Cost: 0.077425 Accuracy 100.00%\n",
      "Epoch  220/1000 Cost: 0.074595 Accuracy 100.00%\n",
      "Epoch  230/1000 Cost: 0.071969 Accuracy 100.00%\n",
      "Epoch  240/1000 Cost: 0.069526 Accuracy 100.00%\n",
      "Epoch  250/1000 Cost: 0.067248 Accuracy 100.00%\n",
      "Epoch  260/1000 Cost: 0.065118 Accuracy 100.00%\n",
      "Epoch  270/1000 Cost: 0.063122 Accuracy 100.00%\n",
      "Epoch  280/1000 Cost: 0.061247 Accuracy 100.00%\n",
      "Epoch  290/1000 Cost: 0.059483 Accuracy 100.00%\n",
      "Epoch  300/1000 Cost: 0.057820 Accuracy 100.00%\n",
      "Epoch  310/1000 Cost: 0.056250 Accuracy 100.00%\n",
      "Epoch  320/1000 Cost: 0.054764 Accuracy 100.00%\n",
      "Epoch  330/1000 Cost: 0.053357 Accuracy 100.00%\n",
      "Epoch  340/1000 Cost: 0.052022 Accuracy 100.00%\n",
      "Epoch  350/1000 Cost: 0.050753 Accuracy 100.00%\n",
      "Epoch  360/1000 Cost: 0.049546 Accuracy 100.00%\n",
      "Epoch  370/1000 Cost: 0.048396 Accuracy 100.00%\n",
      "Epoch  380/1000 Cost: 0.047299 Accuracy 100.00%\n",
      "Epoch  390/1000 Cost: 0.046252 Accuracy 100.00%\n",
      "Epoch  400/1000 Cost: 0.045251 Accuracy 100.00%\n",
      "Epoch  410/1000 Cost: 0.044294 Accuracy 100.00%\n",
      "Epoch  420/1000 Cost: 0.043376 Accuracy 100.00%\n",
      "Epoch  430/1000 Cost: 0.042497 Accuracy 100.00%\n",
      "Epoch  440/1000 Cost: 0.041653 Accuracy 100.00%\n",
      "Epoch  450/1000 Cost: 0.040843 Accuracy 100.00%\n",
      "Epoch  460/1000 Cost: 0.040064 Accuracy 100.00%\n",
      "Epoch  470/1000 Cost: 0.039315 Accuracy 100.00%\n",
      "Epoch  480/1000 Cost: 0.038593 Accuracy 100.00%\n",
      "Epoch  490/1000 Cost: 0.037898 Accuracy 100.00%\n",
      "Epoch  500/1000 Cost: 0.037228 Accuracy 100.00%\n",
      "Epoch  510/1000 Cost: 0.036582 Accuracy 100.00%\n",
      "Epoch  520/1000 Cost: 0.035958 Accuracy 100.00%\n",
      "Epoch  530/1000 Cost: 0.035356 Accuracy 100.00%\n",
      "Epoch  540/1000 Cost: 0.034773 Accuracy 100.00%\n",
      "Epoch  550/1000 Cost: 0.034210 Accuracy 100.00%\n",
      "Epoch  560/1000 Cost: 0.033664 Accuracy 100.00%\n",
      "Epoch  570/1000 Cost: 0.033137 Accuracy 100.00%\n",
      "Epoch  580/1000 Cost: 0.032625 Accuracy 100.00%\n",
      "Epoch  590/1000 Cost: 0.032130 Accuracy 100.00%\n",
      "Epoch  600/1000 Cost: 0.031649 Accuracy 100.00%\n",
      "Epoch  610/1000 Cost: 0.031183 Accuracy 100.00%\n",
      "Epoch  620/1000 Cost: 0.030730 Accuracy 100.00%\n",
      "Epoch  630/1000 Cost: 0.030291 Accuracy 100.00%\n",
      "Epoch  640/1000 Cost: 0.029864 Accuracy 100.00%\n",
      "Epoch  650/1000 Cost: 0.029449 Accuracy 100.00%\n",
      "Epoch  660/1000 Cost: 0.029046 Accuracy 100.00%\n",
      "Epoch  670/1000 Cost: 0.028654 Accuracy 100.00%\n",
      "Epoch  680/1000 Cost: 0.028272 Accuracy 100.00%\n",
      "Epoch  690/1000 Cost: 0.027900 Accuracy 100.00%\n",
      "Epoch  700/1000 Cost: 0.027538 Accuracy 100.00%\n",
      "Epoch  710/1000 Cost: 0.027186 Accuracy 100.00%\n",
      "Epoch  720/1000 Cost: 0.026842 Accuracy 100.00%\n",
      "Epoch  730/1000 Cost: 0.026507 Accuracy 100.00%\n",
      "Epoch  740/1000 Cost: 0.026181 Accuracy 100.00%\n",
      "Epoch  750/1000 Cost: 0.025862 Accuracy 100.00%\n",
      "Epoch  760/1000 Cost: 0.025552 Accuracy 100.00%\n",
      "Epoch  770/1000 Cost: 0.025248 Accuracy 100.00%\n",
      "Epoch  780/1000 Cost: 0.024952 Accuracy 100.00%\n",
      "Epoch  790/1000 Cost: 0.024663 Accuracy 100.00%\n",
      "Epoch  800/1000 Cost: 0.024381 Accuracy 100.00%\n",
      "Epoch  810/1000 Cost: 0.024104 Accuracy 100.00%\n",
      "Epoch  820/1000 Cost: 0.023835 Accuracy 100.00%\n",
      "Epoch  830/1000 Cost: 0.023571 Accuracy 100.00%\n",
      "Epoch  840/1000 Cost: 0.023313 Accuracy 100.00%\n",
      "Epoch  850/1000 Cost: 0.023061 Accuracy 100.00%\n",
      "Epoch  860/1000 Cost: 0.022814 Accuracy 100.00%\n",
      "Epoch  870/1000 Cost: 0.022572 Accuracy 100.00%\n",
      "Epoch  880/1000 Cost: 0.022336 Accuracy 100.00%\n",
      "Epoch  890/1000 Cost: 0.022104 Accuracy 100.00%\n",
      "Epoch  900/1000 Cost: 0.021877 Accuracy 100.00%\n",
      "Epoch  910/1000 Cost: 0.021655 Accuracy 100.00%\n",
      "Epoch  920/1000 Cost: 0.021437 Accuracy 100.00%\n",
      "Epoch  930/1000 Cost: 0.021224 Accuracy 100.00%\n",
      "Epoch  940/1000 Cost: 0.021015 Accuracy 100.00%\n",
      "Epoch  950/1000 Cost: 0.020810 Accuracy 100.00%\n",
      "Epoch  960/1000 Cost: 0.020609 Accuracy 100.00%\n",
      "Epoch  970/1000 Cost: 0.020412 Accuracy 100.00%\n",
      "Epoch  980/1000 Cost: 0.020219 Accuracy 100.00%\n",
      "Epoch  990/1000 Cost: 0.020029 Accuracy 100.00%\n",
      "Epoch 1000/1000 Cost: 0.019843 Accuracy 100.00%\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 10 == 0:\n",
    "        prediction = hypothesis >= torch.FloatTensor([0.5]) # 예측값이 0.5를 넘으면 True로 간주\n",
    "        correct_prediction = prediction.float() == y_train # 실제값과 일치하는 경우만 True로 간주\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도를 계산\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format( # 각 에포크마다 정확도를 출력\n",
    "            epoch, nb_epochs, cost.item(), accuracy * 100,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 [1, 4]일 때의 예측값 : tensor([[0.0057]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 입력 [1, 4]를 선언\n",
    "new_var = torch.FloatTensor([[1.0, 4.0]])\n",
    "# 입력한 값 [1, 4]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
    "pred_y = model(new_var) # forward 연산\n",
    "print(\"훈련 후 입력이 [1, 4]일 때의 예측값 :\", pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False]])\n"
     ]
    }
   ],
   "source": [
    "prediction = pred_y >= torch.FloatTensor([0.5])\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05-03 소프트 맥스 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10b264770>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) 파이토치로 소프트맥스의 비용 함수 구현하기 (로우-레벨)\n",
    "z = torch.FloatTensor([1, 2, 3])\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "hypothesis = F.softmax(z, dim=0)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis.sum() # 총 원소의 값은 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],\n",
       "        [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],\n",
       "        [0.6387, 0.5247, 0.6826, 0.3051, 0.4635]], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.rand(3, 5, requires_grad=True)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2645, 0.1639, 0.1855, 0.2585, 0.1277],\n",
      "        [0.2430, 0.1624, 0.2322, 0.1930, 0.1694],\n",
      "        [0.2226, 0.1986, 0.2326, 0.1594, 0.1868]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = F.softmax(z, dim=1) # 차원을 맞춰주기 위해서 열 방향에 소프트맥스 적용\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "y = torch.randint(5, (3,)).long() # 0이상 5미만의 정수 중에서 3개를 랜덤하게 뽑아 길이 3의 1차원 텐서로 생성 -> (3,)은 정답라벨이 세개\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 레이블에 대해서 원-핫 인코딩 수행\n",
    "y_one_hot = torch.zeros_like(hypothesis) # 모든 원소가 0인 텐서 생성\n",
    "y_one_hot.scatter_(1, y.unsqueeze(1), 1) # scatter의 첫번째 인자로 dim=1에 대해 수행, y가 가리키는 인덱스에 1을 채워넣음, _는 덮어쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4689, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 비용함수: 크로스 엔트로피 함수\n",
    "cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n",
       "        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n",
       "        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]], grad_fn=<LogBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) 파이토치로 소프트맥스의 비용 함수 구현하기 (하이-레벨)\n",
    "# 앞서 Low level\n",
    "torch.log(F.softmax(z, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n",
       "        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n",
       "        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# High level -> log까지 포함\n",
    "F.log_softmax(z, dim=1)     # 출력 결과 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_one_hot * -F.log_softmax(z, dim=1)).sum(dim=1).mean() # 출력 결과 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# High level -> 더 간단하게 하는 법, 실제값을 인자로 사용함\n",
    "F.nll_loss(F.log_softmax(z, dim=1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 더 간단하게 하는 법, 소프트맥스와 로그, NLL을 한꺼번에 수행\n",
    "F.cross_entropy(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F.cross_entropy()는 비용함수에 소프트맥스 함수까지 포함하고 있음을 기억하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 소프트맥스 회귀 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 준비\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10b264770>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [[1, 2, 1, 1],\n",
    "           [2, 1, 3, 2],\n",
    "           [3, 1, 3, 4],\n",
    "           [4, 1, 5, 5],\n",
    "           [1, 7, 5, 5],\n",
    "           [1, 2, 5, 6],\n",
    "           [1, 6, 6, 6],\n",
    "           [1, 7, 7, 7]]\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0] # 레이블은 0, 1, 2 세가지\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# 소프트맥스 회귀 구현하기 - 로우레벨\n",
    "print(x_train.shape) \n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3])\n"
     ]
    }
   ],
   "source": [
    "y_one_hot = torch.zeros((8, 3)) # 8개의 샘플, 3개의 클래스\n",
    "y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\n",
    "print(y_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 초기화\n",
    "W = torch.zeros((4, 3), requires_grad=True)\n",
    "b = torch.zeros((1, 3), requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.098612\n",
      "Epoch  100/1000 Cost: 0.704199\n",
      "Epoch  200/1000 Cost: 0.623000\n",
      "Epoch  300/1000 Cost: 0.565717\n",
      "Epoch  400/1000 Cost: 0.515291\n",
      "Epoch  500/1000 Cost: 0.467662\n",
      "Epoch  600/1000 Cost: 0.421278\n",
      "Epoch  700/1000 Cost: 0.375402\n",
      "Epoch  800/1000 Cost: 0.329766\n",
      "Epoch  900/1000 Cost: 0.285073\n",
      "Epoch 1000/1000 Cost: 0.248155\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # 가설\n",
    "    hypothesis = F.softmax(x_train.matmul(W) + b, dim=1)\n",
    "\n",
    "    # 비용 함수\n",
    "    cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.098612\n",
      "Epoch  100/1000 Cost: 0.704199\n",
      "Epoch  200/1000 Cost: 0.623000\n",
      "Epoch  300/1000 Cost: 0.565717\n",
      "Epoch  400/1000 Cost: 0.515291\n",
      "Epoch  500/1000 Cost: 0.467662\n",
      "Epoch  600/1000 Cost: 0.421278\n",
      "Epoch  700/1000 Cost: 0.375401\n",
      "Epoch  800/1000 Cost: 0.329766\n",
      "Epoch  900/1000 Cost: 0.285073\n",
      "Epoch 1000/1000 Cost: 0.248155\n"
     ]
    }
   ],
   "source": [
    "# 소프트맥스 회귀 구현하기 - 하이레벨\n",
    "# 모델 초기화\n",
    "W = torch.zeros((4, 3), requires_grad=True)\n",
    "b = torch.zeros((1, 3), requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # Cost 계산\n",
    "    z = x_train.matmul(W) + b\n",
    "    cost = F.cross_entropy(z, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소프트맥스 회귀 nn.Module로 구현하기\n",
    "\n",
    "# 모델을 선언 및 초기화. 4개의 특성을 가지고 3개의 클래스로 분류. input_dim=4, output_dim=3.\n",
    "model = nn.Linear(4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.616785\n",
      "Epoch  100/1000 Cost: 0.658891\n",
      "Epoch  200/1000 Cost: 0.573443\n",
      "Epoch  300/1000 Cost: 0.518151\n",
      "Epoch  400/1000 Cost: 0.473265\n",
      "Epoch  500/1000 Cost: 0.433516\n",
      "Epoch  600/1000 Cost: 0.396563\n",
      "Epoch  700/1000 Cost: 0.360914\n",
      "Epoch  800/1000 Cost: 0.325392\n",
      "Epoch  900/1000 Cost: 0.289178\n",
      "Epoch 1000/1000 Cost: 0.254148\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소프트맥스 회귀 클래스로 구현하기\n",
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4, 3) # Output이 3!\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifierModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 2.637636\n",
      "Epoch  100/1000 Cost: 0.647903\n",
      "Epoch  200/1000 Cost: 0.564643\n",
      "Epoch  300/1000 Cost: 0.511043\n",
      "Epoch  400/1000 Cost: 0.467249\n",
      "Epoch  500/1000 Cost: 0.428281\n",
      "Epoch  600/1000 Cost: 0.391924\n",
      "Epoch  700/1000 Cost: 0.356742\n",
      "Epoch  800/1000 Cost: 0.321577\n",
      "Epoch  900/1000 Cost: 0.285617\n",
      "Epoch 1000/1000 Cost: 0.250818\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05-04 소프트맥스 회귀로 MNIST 분류하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1H0Ikp-w1i8WhrK_KatmMEN3Q72BBZacE?usp=share_link"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
